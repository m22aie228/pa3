{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ABHISHEK TAMRAKAR ROLL NUMBER M22AIE228"
      ],
      "metadata": {
        "id": "NXkVyjWbwRdc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.23.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "umQV2LAR5mwc",
        "outputId": "a7e02ec9-a253-4334-8147-ab11e078e50d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.23.2\n",
            "  Downloading numpy-1.23.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.0/17.0 MB\u001b[0m \u001b[31m53.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.25.2\n",
            "    Uninstalling numpy-1.25.2:\n",
            "      Successfully uninstalled numpy-1.25.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "chex 0.1.86 requires numpy>=1.24.1, but you have numpy 1.23.2 which is incompatible.\n",
            "pandas-stubs 2.0.3.230814 requires numpy>=1.25.0; python_version >= \"3.9\", but you have numpy 1.23.2 which is incompatible.\n",
            "tensorflow 2.15.0 requires numpy<2.0.0,>=1.23.5, but you have numpy 1.23.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.23.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "0b48a91e5cf84e46a9985ce8a64da613"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JnPxbgKRSNM9",
        "outputId": "d1856fe9-4ecb-4da2-a4d2-0d80a2006823"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'SSL_Anti-spoofing'...\n",
            "remote: Enumerating objects: 1579, done.\u001b[K\n",
            "remote: Counting objects: 100% (90/90), done.\u001b[K\n",
            "remote: Compressing objects: 100% (42/42), done.\u001b[K\n",
            "remote: Total 1579 (delta 52), reused 82 (delta 48), pack-reused 1489\u001b[K\n",
            "Receiving objects: 100% (1579/1579), 30.57 MiB | 23.29 MiB/s, done.\n",
            "Resolving deltas: 100% (293/293), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/TakHemlata/SSL_Anti-spoofing.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/SSL_Anti-spoofing"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3eJbzSnWTPQ5",
        "outputId": "e87fdc1c-d39b-4ae8-c7bd-49367b4346ed"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/SSL_Anti-spoofing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/SSL_Anti-spoofing/fairseq-a54021305d6b3c4c5959ac9395135f63202db8f1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hffk9kvWTWBQ",
        "outputId": "9d8daa8b-4022-4272-b573-c6e345cadba2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/SSL_Anti-spoofing/fairseq-a54021305d6b3c4c5959ac9395135f63202db8f1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --editable ./"
      ],
      "metadata": {
        "id": "7eh0jZRRTkZE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3405a2b-af95-4ace-d894-7720967590db"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obtaining file:///content/SSL_Anti-spoofing/fairseq-a54021305d6b3c4c5959ac9395135f63202db8f1\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.10/dist-packages (from fairseq==1.0.0a0+4acaa61) (1.16.0)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from fairseq==1.0.0a0+4acaa61) (3.0.10)\n",
            "Collecting hydra-core<1.1,>=1.0.7 (from fairseq==1.0.0a0+4acaa61)\n",
            "  Downloading hydra_core-1.0.7-py3-none-any.whl (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting omegaconf<2.1 (from fairseq==1.0.0a0+4acaa61)\n",
            "  Downloading omegaconf-2.0.6-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from fairseq==1.0.0a0+4acaa61) (2023.12.25)\n",
            "Collecting sacrebleu>=1.4.12 (from fairseq==1.0.0a0+4acaa61)\n",
            "  Downloading sacrebleu-2.4.2-py3-none-any.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from fairseq==1.0.0a0+4acaa61) (2.2.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fairseq==1.0.0a0+4acaa61) (4.66.2)\n",
            "Collecting bitarray (from fairseq==1.0.0a0+4acaa61)\n",
            "  Downloading bitarray-2.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.3/288.3 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchaudio>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from fairseq==1.0.0a0+4acaa61) (2.2.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fairseq==1.0.0a0+4acaa61) (1.23.2)\n",
            "Collecting antlr4-python3-runtime==4.8 (from hydra-core<1.1,>=1.0.7->fairseq==1.0.0a0+4acaa61)\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.4/112.4 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq==1.0.0a0+4acaa61) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq==1.0.0a0+4acaa61) (4.11.0)\n",
            "Collecting portalocker (from sacrebleu>=1.4.12->fairseq==1.0.0a0+4acaa61)\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq==1.0.0a0+4acaa61) (0.9.0)\n",
            "Collecting colorama (from sacrebleu>=1.4.12->fairseq==1.0.0a0+4acaa61)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq==1.0.0a0+4acaa61) (4.9.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->fairseq==1.0.0a0+4acaa61) (3.14.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->fairseq==1.0.0a0+4acaa61) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->fairseq==1.0.0a0+4acaa61) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq==1.0.0a0+4acaa61) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->fairseq==1.0.0a0+4acaa61) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->fairseq==1.0.0a0+4acaa61)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->fairseq==1.0.0a0+4acaa61)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->fairseq==1.0.0a0+4acaa61)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->fairseq==1.0.0a0+4acaa61)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->fairseq==1.0.0a0+4acaa61)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->fairseq==1.0.0a0+4acaa61)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->fairseq==1.0.0a0+4acaa61)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->fairseq==1.0.0a0+4acaa61)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->fairseq==1.0.0a0+4acaa61)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch->fairseq==1.0.0a0+4acaa61)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->fairseq==1.0.0a0+4acaa61)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq==1.0.0a0+4acaa61) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->fairseq==1.0.0a0+4acaa61)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi->fairseq==1.0.0a0+4acaa61) (2.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->fairseq==1.0.0a0+4acaa61) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->fairseq==1.0.0a0+4acaa61) (1.3.0)\n",
            "Building wheels for collected packages: fairseq, antlr4-python3-runtime\n",
            "  Building editable for fairseq (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairseq: filename=fairseq-1.0.0a0+4acaa61-0.editable-cp310-cp310-linux_x86_64.whl size=9253 sha256=b07b87c0ee3880c227509ba8fadf8c03157eac1623104de01cf136b06471540e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-h07ddtyt/wheels/7e/e7/f3/b9dc85bed2878480a562707319b545b2d7bbf958307b8eddc5\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141211 sha256=b26be89004c37f28cfb2a6639516f787c6a0daaff1008bb5f805b8222f4744c0\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/20/bd/e1477d664f22d99989fd28ee1a43d6633dddb5cb9e801350d5\n",
            "Successfully built fairseq antlr4-python3-runtime\n",
            "Installing collected packages: bitarray, antlr4-python3-runtime, portalocker, omegaconf, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, colorama, sacrebleu, nvidia-cusparse-cu12, nvidia-cudnn-cu12, hydra-core, nvidia-cusolver-cu12, fairseq\n",
            "Successfully installed antlr4-python3-runtime-4.8 bitarray-2.9.2 colorama-0.4.6 fairseq-1.0.0a0+4acaa61 hydra-core-1.0.7 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 omegaconf-2.0.6 portalocker-2.8.2 sacrebleu-2.4.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHm3ncaoUS9e",
        "outputId": "a5059e7d-916f-47f9-8e75-7fe0dd1917a7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat May  4 17:00:31 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   32C    P0              43W / 400W |      2MiB / 40960MiB |      0%      Default |\n",
            "|                                         |                      |             Disabled |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!cd /content/SSL_Anti-spoofing/\n",
        "!wget https://dl.fbaipublicfiles.com/fairseq/wav2vec/xlsr2_300m.pt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kyOqsuM0V6aU",
        "outputId": "2db5d908-a910-45dd-ea0d-d73d59281884"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/SSL_Anti-spoofing/fairseq-a54021305d6b3c4c5959ac9395135f63202db8f1\n",
            "--2024-05-04 17:00:41--  https://dl.fbaipublicfiles.com/fairseq/wav2vec/xlsr2_300m.pt\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 3.162.163.34, 3.162.163.11, 3.162.163.51, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|3.162.163.34|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3808868242 (3.5G) [binary/octet-stream]\n",
            "Saving to: ‘xlsr2_300m.pt’\n",
            "\n",
            "xlsr2_300m.pt       100%[===================>]   3.55G  56.6MB/s    in 69s     \n",
            "\n",
            "2024-05-04 17:01:51 (52.3 MB/s) - ‘xlsr2_300m.pt’ saved [3808868242/3808868242]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from typing import Union\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "import fairseq\n",
        "\n",
        "\n",
        "___author__ = \"Hemlata Tak\"\n",
        "__email__ = \"tak@eurecom.fr\"\n",
        "\n",
        "############################\n",
        "## FOR fine-tuned SSL MODEL\n",
        "############################\n",
        "\n",
        "\n",
        "class SSLModel(nn.Module):\n",
        "    def __init__(self,device):\n",
        "        super(SSLModel, self).__init__()\n",
        "\n",
        "        cp_path = '/content/SSL_Anti-spoofing/fairseq-a54021305d6b3c4c5959ac9395135f63202db8f1/xlsr2_300m.pt'   # Change the pre-trained XLSR model path.\n",
        "        model, cfg, task = fairseq.checkpoint_utils.load_model_ensemble_and_task([cp_path])\n",
        "        self.model = model[0]\n",
        "        self.device=device\n",
        "        self.out_dim = 1024\n",
        "        return\n",
        "\n",
        "    def extract_feat(self, input_data):\n",
        "\n",
        "        # put the model to GPU if it not there\n",
        "        if next(self.model.parameters()).device != input_data.device \\\n",
        "           or next(self.model.parameters()).dtype != input_data.dtype:\n",
        "            self.model.to(input_data.device, dtype=input_data.dtype)\n",
        "            self.model.train()\n",
        "\n",
        "\n",
        "        if True:\n",
        "            # input should be in shape (batch, length)\n",
        "            if input_data.ndim == 3:\n",
        "                input_tmp = input_data[:, :, 0]\n",
        "            else:\n",
        "                input_tmp = input_data\n",
        "\n",
        "            # [batch, length, dim]\n",
        "            emb = self.model(input_tmp, mask=False, features_only=True)['x']\n",
        "        return emb\n",
        "\n",
        "\n",
        "#---------AASIST back-end------------------------#\n",
        "''' Jee-weon Jung, Hee-Soo Heo, Hemlata Tak, Hye-jin Shim, Joon Son Chung, Bong-Jin Lee, Ha-Jin Yu and Nicholas Evans.\n",
        "    AASIST: Audio Anti-Spoofing Using Integrated Spectro-Temporal Graph Attention Networks.\n",
        "    In Proc. ICASSP 2022, pp: 6367--6371.'''\n",
        "\n",
        "\n",
        "class GraphAttentionLayer(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, **kwargs):\n",
        "        super().__init__()\n",
        "\n",
        "        # attention map\n",
        "        self.att_proj = nn.Linear(in_dim, out_dim)\n",
        "        self.att_weight = self._init_new_params(out_dim, 1)\n",
        "\n",
        "        # project\n",
        "        self.proj_with_att = nn.Linear(in_dim, out_dim)\n",
        "        self.proj_without_att = nn.Linear(in_dim, out_dim)\n",
        "\n",
        "        # batch norm\n",
        "        self.bn = nn.BatchNorm1d(out_dim)\n",
        "\n",
        "        # dropout for inputs\n",
        "        self.input_drop = nn.Dropout(p=0.2)\n",
        "\n",
        "        # activate\n",
        "        self.act = nn.SELU(inplace=True)\n",
        "\n",
        "        # temperature\n",
        "        self.temp = 1.\n",
        "        if \"temperature\" in kwargs:\n",
        "            self.temp = kwargs[\"temperature\"]\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        x   :(#bs, #node, #dim)\n",
        "        '''\n",
        "        # apply input dropout\n",
        "        x = self.input_drop(x)\n",
        "\n",
        "        # derive attention map\n",
        "        att_map = self._derive_att_map(x)\n",
        "\n",
        "        # projection\n",
        "        x = self._project(x, att_map)\n",
        "\n",
        "        # apply batch norm\n",
        "        x = self._apply_BN(x)\n",
        "        x = self.act(x)\n",
        "        return x\n",
        "\n",
        "    def _pairwise_mul_nodes(self, x):\n",
        "        '''\n",
        "        Calculates pairwise multiplication of nodes.\n",
        "        - for attention map\n",
        "        x           :(#bs, #node, #dim)\n",
        "        out_shape   :(#bs, #node, #node, #dim)\n",
        "        '''\n",
        "\n",
        "        nb_nodes = x.size(1)\n",
        "        x = x.unsqueeze(2).expand(-1, -1, nb_nodes, -1)\n",
        "        x_mirror = x.transpose(1, 2)\n",
        "\n",
        "        return x * x_mirror\n",
        "\n",
        "    def _derive_att_map(self, x):\n",
        "        '''\n",
        "        x           :(#bs, #node, #dim)\n",
        "        out_shape   :(#bs, #node, #node, 1)\n",
        "        '''\n",
        "        att_map = self._pairwise_mul_nodes(x)\n",
        "        # size: (#bs, #node, #node, #dim_out)\n",
        "        att_map = torch.tanh(self.att_proj(att_map))\n",
        "        # size: (#bs, #node, #node, 1)\n",
        "        att_map = torch.matmul(att_map, self.att_weight)\n",
        "\n",
        "        # apply temperature\n",
        "        att_map = att_map / self.temp\n",
        "\n",
        "        att_map = F.softmax(att_map, dim=-2)\n",
        "\n",
        "        return att_map\n",
        "\n",
        "    def _project(self, x, att_map):\n",
        "        x1 = self.proj_with_att(torch.matmul(att_map.squeeze(-1), x))\n",
        "        x2 = self.proj_without_att(x)\n",
        "\n",
        "        return x1 + x2\n",
        "\n",
        "    def _apply_BN(self, x):\n",
        "        org_size = x.size()\n",
        "        x = x.view(-1, org_size[-1])\n",
        "        x = self.bn(x)\n",
        "        x = x.view(org_size)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def _init_new_params(self, *size):\n",
        "        out = nn.Parameter(torch.FloatTensor(*size))\n",
        "        nn.init.xavier_normal_(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class HtrgGraphAttentionLayer(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, **kwargs):\n",
        "        super().__init__()\n",
        "\n",
        "        self.proj_type1 = nn.Linear(in_dim, in_dim)\n",
        "        self.proj_type2 = nn.Linear(in_dim, in_dim)\n",
        "\n",
        "        # attention map\n",
        "        self.att_proj = nn.Linear(in_dim, out_dim)\n",
        "        self.att_projM = nn.Linear(in_dim, out_dim)\n",
        "\n",
        "        self.att_weight11 = self._init_new_params(out_dim, 1)\n",
        "        self.att_weight22 = self._init_new_params(out_dim, 1)\n",
        "        self.att_weight12 = self._init_new_params(out_dim, 1)\n",
        "        self.att_weightM = self._init_new_params(out_dim, 1)\n",
        "\n",
        "        # project\n",
        "        self.proj_with_att = nn.Linear(in_dim, out_dim)\n",
        "        self.proj_without_att = nn.Linear(in_dim, out_dim)\n",
        "\n",
        "        self.proj_with_attM = nn.Linear(in_dim, out_dim)\n",
        "        self.proj_without_attM = nn.Linear(in_dim, out_dim)\n",
        "\n",
        "        # batch norm\n",
        "        self.bn = nn.BatchNorm1d(out_dim)\n",
        "\n",
        "        # dropout for inputs\n",
        "        self.input_drop = nn.Dropout(p=0.2)\n",
        "\n",
        "        # activate\n",
        "        self.act = nn.SELU(inplace=True)\n",
        "\n",
        "        # temperature\n",
        "        self.temp = 1.\n",
        "        if \"temperature\" in kwargs:\n",
        "            self.temp = kwargs[\"temperature\"]\n",
        "\n",
        "    def forward(self, x1, x2, master=None):\n",
        "        '''\n",
        "        x1  :(#bs, #node, #dim)\n",
        "        x2  :(#bs, #node, #dim)\n",
        "        '''\n",
        "        #print('x1',x1.shape)\n",
        "        #print('x2',x2.shape)\n",
        "        num_type1 = x1.size(1)\n",
        "        num_type2 = x2.size(1)\n",
        "        #print('num_type1',num_type1)\n",
        "        #print('num_type2',num_type2)\n",
        "        x1 = self.proj_type1(x1)\n",
        "        #print('proj_type1',x1.shape)\n",
        "        x2 = self.proj_type2(x2)\n",
        "        #print('proj_type2',x2.shape)\n",
        "        x = torch.cat([x1, x2], dim=1)\n",
        "        #print('Concat x1 and x2',x.shape)\n",
        "\n",
        "        if master is None:\n",
        "            master = torch.mean(x, dim=1, keepdim=True)\n",
        "            #print('master',master.shape)\n",
        "        # apply input dropout\n",
        "        x = self.input_drop(x)\n",
        "\n",
        "        # derive attention map\n",
        "        att_map = self._derive_att_map(x, num_type1, num_type2)\n",
        "        #print('master',master.shape)\n",
        "        # directional edge for master node\n",
        "        master = self._update_master(x, master)\n",
        "        #print('master',master.shape)\n",
        "        # projection\n",
        "        x = self._project(x, att_map)\n",
        "        #print('proj x',x.shape)\n",
        "        # apply batch norm\n",
        "        x = self._apply_BN(x)\n",
        "        x = self.act(x)\n",
        "\n",
        "        x1 = x.narrow(1, 0, num_type1)\n",
        "        #print('x1',x1.shape)\n",
        "        x2 = x.narrow(1, num_type1, num_type2)\n",
        "        #print('x2',x2.shape)\n",
        "        return x1, x2, master\n",
        "\n",
        "    def _update_master(self, x, master):\n",
        "\n",
        "        att_map = self._derive_att_map_master(x, master)\n",
        "        master = self._project_master(x, master, att_map)\n",
        "\n",
        "        return master\n",
        "\n",
        "    def _pairwise_mul_nodes(self, x):\n",
        "        '''\n",
        "        Calculates pairwise multiplication of nodes.\n",
        "        - for attention map\n",
        "        x           :(#bs, #node, #dim)\n",
        "        out_shape   :(#bs, #node, #node, #dim)\n",
        "        '''\n",
        "\n",
        "        nb_nodes = x.size(1)\n",
        "        x = x.unsqueeze(2).expand(-1, -1, nb_nodes, -1)\n",
        "        x_mirror = x.transpose(1, 2)\n",
        "\n",
        "        return x * x_mirror\n",
        "\n",
        "    def _derive_att_map_master(self, x, master):\n",
        "        '''\n",
        "        x           :(#bs, #node, #dim)\n",
        "        out_shape   :(#bs, #node, #node, 1)\n",
        "        '''\n",
        "        att_map = x * master\n",
        "        att_map = torch.tanh(self.att_projM(att_map))\n",
        "\n",
        "        att_map = torch.matmul(att_map, self.att_weightM)\n",
        "\n",
        "        # apply temperature\n",
        "        att_map = att_map / self.temp\n",
        "\n",
        "        att_map = F.softmax(att_map, dim=-2)\n",
        "\n",
        "        return att_map\n",
        "\n",
        "    def _derive_att_map(self, x, num_type1, num_type2):\n",
        "        '''\n",
        "        x           :(#bs, #node, #dim)\n",
        "        out_shape   :(#bs, #node, #node, 1)\n",
        "        '''\n",
        "        att_map = self._pairwise_mul_nodes(x)\n",
        "        # size: (#bs, #node, #node, #dim_out)\n",
        "        att_map = torch.tanh(self.att_proj(att_map))\n",
        "        # size: (#bs, #node, #node, 1)\n",
        "\n",
        "        att_board = torch.zeros_like(att_map[:, :, :, 0]).unsqueeze(-1)\n",
        "\n",
        "        att_board[:, :num_type1, :num_type1, :] = torch.matmul(\n",
        "            att_map[:, :num_type1, :num_type1, :], self.att_weight11)\n",
        "        att_board[:, num_type1:, num_type1:, :] = torch.matmul(\n",
        "            att_map[:, num_type1:, num_type1:, :], self.att_weight22)\n",
        "        att_board[:, :num_type1, num_type1:, :] = torch.matmul(\n",
        "            att_map[:, :num_type1, num_type1:, :], self.att_weight12)\n",
        "        att_board[:, num_type1:, :num_type1, :] = torch.matmul(\n",
        "            att_map[:, num_type1:, :num_type1, :], self.att_weight12)\n",
        "\n",
        "        att_map = att_board\n",
        "\n",
        "\n",
        "\n",
        "        # apply temperature\n",
        "        att_map = att_map / self.temp\n",
        "\n",
        "        att_map = F.softmax(att_map, dim=-2)\n",
        "\n",
        "        return att_map\n",
        "\n",
        "    def _project(self, x, att_map):\n",
        "        x1 = self.proj_with_att(torch.matmul(att_map.squeeze(-1), x))\n",
        "        x2 = self.proj_without_att(x)\n",
        "\n",
        "        return x1 + x2\n",
        "\n",
        "    def _project_master(self, x, master, att_map):\n",
        "\n",
        "        x1 = self.proj_with_attM(torch.matmul(\n",
        "            att_map.squeeze(-1).unsqueeze(1), x))\n",
        "        x2 = self.proj_without_attM(master)\n",
        "\n",
        "        return x1 + x2\n",
        "\n",
        "    def _apply_BN(self, x):\n",
        "        org_size = x.size()\n",
        "        x = x.view(-1, org_size[-1])\n",
        "        x = self.bn(x)\n",
        "        x = x.view(org_size)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def _init_new_params(self, *size):\n",
        "        out = nn.Parameter(torch.FloatTensor(*size))\n",
        "        nn.init.xavier_normal_(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class GraphPool(nn.Module):\n",
        "    def __init__(self, k: float, in_dim: int, p: Union[float, int]):\n",
        "        super().__init__()\n",
        "        self.k = k\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.proj = nn.Linear(in_dim, 1)\n",
        "        self.drop = nn.Dropout(p=p) if p > 0 else nn.Identity()\n",
        "        self.in_dim = in_dim\n",
        "\n",
        "    def forward(self, h):\n",
        "        Z = self.drop(h)\n",
        "        weights = self.proj(Z)\n",
        "        scores = self.sigmoid(weights)\n",
        "        new_h = self.top_k_graph(scores, h, self.k)\n",
        "\n",
        "        return new_h\n",
        "\n",
        "    def top_k_graph(self, scores, h, k):\n",
        "        \"\"\"\n",
        "        args\n",
        "        =====\n",
        "        scores: attention-based weights (#bs, #node, 1)\n",
        "        h: graph data (#bs, #node, #dim)\n",
        "        k: ratio of remaining nodes, (float)\n",
        "        returns\n",
        "        =====\n",
        "        h: graph pool applied data (#bs, #node', #dim)\n",
        "        \"\"\"\n",
        "        _, n_nodes, n_feat = h.size()\n",
        "        n_nodes = max(int(n_nodes * k), 1)\n",
        "        _, idx = torch.topk(scores, n_nodes, dim=1)\n",
        "        idx = idx.expand(-1, -1, n_feat)\n",
        "\n",
        "        h = h * scores\n",
        "        h = torch.gather(h, 1, idx)\n",
        "\n",
        "        return h\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Residual_block(nn.Module):\n",
        "    def __init__(self, nb_filts, first=False):\n",
        "        super().__init__()\n",
        "        self.first = first\n",
        "\n",
        "        if not self.first:\n",
        "            self.bn1 = nn.BatchNorm2d(num_features=nb_filts[0])\n",
        "        self.conv1 = nn.Conv2d(in_channels=nb_filts[0],\n",
        "                               out_channels=nb_filts[1],\n",
        "                               kernel_size=(2, 3),\n",
        "                               padding=(1, 1),\n",
        "                               stride=1)\n",
        "        self.selu = nn.SELU(inplace=True)\n",
        "\n",
        "        self.bn2 = nn.BatchNorm2d(num_features=nb_filts[1])\n",
        "        self.conv2 = nn.Conv2d(in_channels=nb_filts[1],\n",
        "                               out_channels=nb_filts[1],\n",
        "                               kernel_size=(2, 3),\n",
        "                               padding=(0, 1),\n",
        "                               stride=1)\n",
        "\n",
        "        if nb_filts[0] != nb_filts[1]:\n",
        "            self.downsample = True\n",
        "            self.conv_downsample = nn.Conv2d(in_channels=nb_filts[0],\n",
        "                                             out_channels=nb_filts[1],\n",
        "                                             padding=(0, 1),\n",
        "                                             kernel_size=(1, 3),\n",
        "                                             stride=1)\n",
        "\n",
        "        else:\n",
        "            self.downsample = False\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        if not self.first:\n",
        "            out = self.bn1(x)\n",
        "            out = self.selu(out)\n",
        "        else:\n",
        "            out = x\n",
        "\n",
        "        #print('out',out.shape)\n",
        "        out = self.conv1(x)\n",
        "\n",
        "        #print('aft conv1 out',out.shape)\n",
        "        out = self.bn2(out)\n",
        "        out = self.selu(out)\n",
        "        # print('out',out.shape)\n",
        "        out = self.conv2(out)\n",
        "        #print('conv2 out',out.shape)\n",
        "\n",
        "        if self.downsample:\n",
        "            identity = self.conv_downsample(identity)\n",
        "\n",
        "        out += identity\n",
        "        #out = self.mp(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self,device):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "\n",
        "        # AASIST parameters\n",
        "        filts = [128, [1, 32], [32, 32], [32, 64], [64, 64]]\n",
        "        gat_dims = [64, 32]\n",
        "        pool_ratios = [0.5, 0.5, 0.5, 0.5]\n",
        "        temperatures =  [2.0, 2.0, 100.0, 100.0]\n",
        "\n",
        "\n",
        "        ####\n",
        "        # create network wav2vec 2.0\n",
        "        ####\n",
        "        self.ssl_model = SSLModel(self.device)\n",
        "        self.LL = nn.Linear(self.ssl_model.out_dim, 128)\n",
        "\n",
        "        self.first_bn = nn.BatchNorm2d(num_features=1)\n",
        "        self.first_bn1 = nn.BatchNorm2d(num_features=64)\n",
        "        self.drop = nn.Dropout(0.5, inplace=True)\n",
        "        self.drop_way = nn.Dropout(0.2, inplace=True)\n",
        "        self.selu = nn.SELU(inplace=True)\n",
        "\n",
        "        # RawNet2 encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Sequential(Residual_block(nb_filts=filts[1], first=True)),\n",
        "            nn.Sequential(Residual_block(nb_filts=filts[2])),\n",
        "            nn.Sequential(Residual_block(nb_filts=filts[3])),\n",
        "            nn.Sequential(Residual_block(nb_filts=filts[4])),\n",
        "            nn.Sequential(Residual_block(nb_filts=filts[4])),\n",
        "            nn.Sequential(Residual_block(nb_filts=filts[4])))\n",
        "\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, kernel_size=(1,1)),\n",
        "            nn.SELU(inplace=True),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.Conv2d(128, 64, kernel_size=(1,1)),\n",
        "\n",
        "        )\n",
        "        # position encoding\n",
        "        self.pos_S = nn.Parameter(torch.randn(1, 42, filts[-1][-1]))\n",
        "\n",
        "        self.master1 = nn.Parameter(torch.randn(1, 1, gat_dims[0]))\n",
        "        self.master2 = nn.Parameter(torch.randn(1, 1, gat_dims[0]))\n",
        "\n",
        "        # Graph module\n",
        "        self.GAT_layer_S = GraphAttentionLayer(filts[-1][-1],\n",
        "                                               gat_dims[0],\n",
        "                                               temperature=temperatures[0])\n",
        "        self.GAT_layer_T = GraphAttentionLayer(filts[-1][-1],\n",
        "                                               gat_dims[0],\n",
        "                                               temperature=temperatures[1])\n",
        "        # HS-GAL layer\n",
        "        self.HtrgGAT_layer_ST11 = HtrgGraphAttentionLayer(\n",
        "            gat_dims[0], gat_dims[1], temperature=temperatures[2])\n",
        "        self.HtrgGAT_layer_ST12 = HtrgGraphAttentionLayer(\n",
        "            gat_dims[1], gat_dims[1], temperature=temperatures[2])\n",
        "        self.HtrgGAT_layer_ST21 = HtrgGraphAttentionLayer(\n",
        "            gat_dims[0], gat_dims[1], temperature=temperatures[2])\n",
        "        self.HtrgGAT_layer_ST22 = HtrgGraphAttentionLayer(\n",
        "            gat_dims[1], gat_dims[1], temperature=temperatures[2])\n",
        "\n",
        "        # Graph pooling layers\n",
        "        self.pool_S = GraphPool(pool_ratios[0], gat_dims[0], 0.3)\n",
        "        self.pool_T = GraphPool(pool_ratios[1], gat_dims[0], 0.3)\n",
        "        self.pool_hS1 = GraphPool(pool_ratios[2], gat_dims[1], 0.3)\n",
        "        self.pool_hT1 = GraphPool(pool_ratios[2], gat_dims[1], 0.3)\n",
        "\n",
        "        self.pool_hS2 = GraphPool(pool_ratios[2], gat_dims[1], 0.3)\n",
        "        self.pool_hT2 = GraphPool(pool_ratios[2], gat_dims[1], 0.3)\n",
        "\n",
        "        self.out_layer = nn.Linear(5 * gat_dims[1], 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #-------pre-trained Wav2vec model fine tunning ------------------------##\n",
        "        x_ssl_feat = self.ssl_model.extract_feat(x.squeeze(-1))\n",
        "        x = self.LL(x_ssl_feat) #(bs,frame_number,feat_out_dim)\n",
        "\n",
        "        # post-processing on front-end features\n",
        "        x = x.transpose(1, 2)   #(bs,feat_out_dim,frame_number)\n",
        "        x = x.unsqueeze(dim=1) # add channel\n",
        "        x = F.max_pool2d(x, (3, 3))\n",
        "        x = self.first_bn(x)\n",
        "        x = self.selu(x)\n",
        "\n",
        "        # RawNet2-based encoder\n",
        "        x = self.encoder(x)\n",
        "        x = self.first_bn1(x)\n",
        "        x = self.selu(x)\n",
        "\n",
        "        w = self.attention(x)\n",
        "\n",
        "        #------------SA for spectral feature-------------#\n",
        "        w1 = F.softmax(w,dim=-1)\n",
        "        m = torch.sum(x * w1, dim=-1)\n",
        "        e_S = m.transpose(1, 2) + self.pos_S\n",
        "\n",
        "        # graph module layer\n",
        "        gat_S = self.GAT_layer_S(e_S)\n",
        "        out_S = self.pool_S(gat_S)  # (#bs, #node, #dim)\n",
        "\n",
        "        #------------SA for temporal feature-------------#\n",
        "        w2 = F.softmax(w,dim=-2)\n",
        "        m1 = torch.sum(x * w2, dim=-2)\n",
        "\n",
        "        e_T = m1.transpose(1, 2)\n",
        "\n",
        "        # graph module layer\n",
        "        gat_T = self.GAT_layer_T(e_T)\n",
        "        out_T = self.pool_T(gat_T)\n",
        "\n",
        "        # learnable master node\n",
        "        master1 = self.master1.expand(x.size(0), -1, -1)\n",
        "        master2 = self.master2.expand(x.size(0), -1, -1)\n",
        "\n",
        "        # inference 1\n",
        "        out_T1, out_S1, master1 = self.HtrgGAT_layer_ST11(\n",
        "            out_T, out_S, master=self.master1)\n",
        "\n",
        "        out_S1 = self.pool_hS1(out_S1)\n",
        "        out_T1 = self.pool_hT1(out_T1)\n",
        "\n",
        "        out_T_aug, out_S_aug, master_aug = self.HtrgGAT_layer_ST12(\n",
        "            out_T1, out_S1, master=master1)\n",
        "        out_T1 = out_T1 + out_T_aug\n",
        "        out_S1 = out_S1 + out_S_aug\n",
        "        master1 = master1 + master_aug\n",
        "\n",
        "        # inference 2\n",
        "        out_T2, out_S2, master2 = self.HtrgGAT_layer_ST21(\n",
        "            out_T, out_S, master=self.master2)\n",
        "        out_S2 = self.pool_hS2(out_S2)\n",
        "        out_T2 = self.pool_hT2(out_T2)\n",
        "\n",
        "        out_T_aug, out_S_aug, master_aug = self.HtrgGAT_layer_ST22(\n",
        "            out_T2, out_S2, master=master2)\n",
        "        out_T2 = out_T2 + out_T_aug\n",
        "        out_S2 = out_S2 + out_S_aug\n",
        "        master2 = master2 + master_aug\n",
        "\n",
        "        out_T1 = self.drop_way(out_T1)\n",
        "        out_T2 = self.drop_way(out_T2)\n",
        "        out_S1 = self.drop_way(out_S1)\n",
        "        out_S2 = self.drop_way(out_S2)\n",
        "        master1 = self.drop_way(master1)\n",
        "        master2 = self.drop_way(master2)\n",
        "\n",
        "        out_T = torch.max(out_T1, out_T2)\n",
        "        out_S = torch.max(out_S1, out_S2)\n",
        "        master = torch.max(master1, master2)\n",
        "\n",
        "        # Readout operation\n",
        "        T_max, _ = torch.max(torch.abs(out_T), dim=1)\n",
        "        T_avg = torch.mean(out_T, dim=1)\n",
        "\n",
        "        S_max, _ = torch.max(torch.abs(out_S), dim=1)\n",
        "        S_avg = torch.mean(out_S, dim=1)\n",
        "\n",
        "        last_hidden = torch.cat(\n",
        "            [T_max, T_avg, S_max, S_avg, master.squeeze(1)], dim=1)\n",
        "\n",
        "        last_hidden = self.drop(last_hidden)\n",
        "        output = self.out_layer(last_hidden)\n",
        "\n",
        "        return output\n",
        "\n"
      ],
      "metadata": {
        "id": "7zmLzweDUSQw"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gdown"
      ],
      "metadata": {
        "id": "nsqJXuP-aCAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download pretrained model LA_model.pth\n",
        "!pwd\n",
        "!cd /content/\n",
        "\n",
        "import gdown\n",
        "\n",
        "# https://drive.google.com/file/d/11vFBNKzYUtWqdK358_JEygawFzmmaZjO/view?usp=drive_link\n",
        "\n",
        "file_id = \"11vFBNKzYUtWqdK358_JEygawFzmmaZjO\"  # Replace this with your file's ID\n",
        "output_file = \"LA_model.pth\"  # Replace \"data_file.ext\" with the desired output filename and extension\n",
        "\n",
        "gdown.download(f\"https://drive.google.com/uc?id={file_id}\", output_file)"
      ],
      "metadata": {
        "id": "Uqh-Y-j5cpD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load SSL W2V model trained for LA i.e LA_model.pth\n",
        "import torch\n",
        "import torchaudio\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = Model(device)\n",
        "\n",
        "model.load_state_dict(torch.load('/content/SSL_Anti-spoofing/fairseq-a54021305d6b3c4c5959ac9395135f63202db8f1/LA_model.pth', map_location=device))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n"
      ],
      "metadata": {
        "id": "7Yxks6ixanzr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83595358-240a-4d79-82d1-29456044690d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
            "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Model(\n",
              "  (ssl_model): SSLModel(\n",
              "    (model): Wav2Vec2Model(\n",
              "      (feature_extractor): ConvFeatureExtractionModel(\n",
              "        (conv_layers): ModuleList(\n",
              "          (0): Sequential(\n",
              "            (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,))\n",
              "            (1): Dropout(p=0.0, inplace=False)\n",
              "            (2): Sequential(\n",
              "              (0): TransposeLast()\n",
              "              (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "              (2): TransposeLast()\n",
              "            )\n",
              "            (3): GELU(approximate='none')\n",
              "          )\n",
              "          (1-4): 4 x Sequential(\n",
              "            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\n",
              "            (1): Dropout(p=0.0, inplace=False)\n",
              "            (2): Sequential(\n",
              "              (0): TransposeLast()\n",
              "              (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "              (2): TransposeLast()\n",
              "            )\n",
              "            (3): GELU(approximate='none')\n",
              "          )\n",
              "          (5-6): 2 x Sequential(\n",
              "            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,))\n",
              "            (1): Dropout(p=0.0, inplace=False)\n",
              "            (2): Sequential(\n",
              "              (0): TransposeLast()\n",
              "              (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "              (2): TransposeLast()\n",
              "            )\n",
              "            (3): GELU(approximate='none')\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (post_extract_proj): Linear(in_features=512, out_features=1024, bias=True)\n",
              "      (dropout_input): Dropout(p=0.0, inplace=False)\n",
              "      (dropout_features): Dropout(p=0.0, inplace=False)\n",
              "      (quantizer): GumbelVectorQuantizer(\n",
              "        (weight_proj): Linear(in_features=512, out_features=640, bias=True)\n",
              "      )\n",
              "      (project_q): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (encoder): TransformerEncoder(\n",
              "        (pos_conv): Sequential(\n",
              "          (0): Conv1d(1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)\n",
              "          (1): SamePad()\n",
              "          (2): GELU(approximate='none')\n",
              "        )\n",
              "        (layers): ModuleList(\n",
              "          (0-23): 24 x TransformerSentenceEncoderLayer(\n",
              "            (self_attn): MultiheadAttention(\n",
              "              (dropout_module): FairseqDropout()\n",
              "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            )\n",
              "            (dropout1): Dropout(p=0.0, inplace=False)\n",
              "            (dropout2): Dropout(p=0.0, inplace=False)\n",
              "            (dropout3): Dropout(p=0.0, inplace=False)\n",
              "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "        )\n",
              "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (final_proj): Linear(in_features=1024, out_features=768, bias=True)\n",
              "    )\n",
              "  )\n",
              "  (LL): Linear(in_features=1024, out_features=128, bias=True)\n",
              "  (first_bn): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (first_bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (drop): Dropout(p=0.5, inplace=True)\n",
              "  (drop_way): Dropout(p=0.2, inplace=True)\n",
              "  (selu): SELU(inplace=True)\n",
              "  (encoder): Sequential(\n",
              "    (0): Sequential(\n",
              "      (0): Residual_block(\n",
              "        (conv1): Conv2d(1, 32, kernel_size=(2, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (selu): SELU(inplace=True)\n",
              "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(2, 3), stride=(1, 1), padding=(0, 1))\n",
              "        (conv_downsample): Conv2d(1, 32, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
              "      )\n",
              "    )\n",
              "    (1): Sequential(\n",
              "      (0): Residual_block(\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv1): Conv2d(32, 32, kernel_size=(2, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (selu): SELU(inplace=True)\n",
              "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(2, 3), stride=(1, 1), padding=(0, 1))\n",
              "      )\n",
              "    )\n",
              "    (2): Sequential(\n",
              "      (0): Residual_block(\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv1): Conv2d(32, 64, kernel_size=(2, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (selu): SELU(inplace=True)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(2, 3), stride=(1, 1), padding=(0, 1))\n",
              "        (conv_downsample): Conv2d(32, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
              "      )\n",
              "    )\n",
              "    (3): Sequential(\n",
              "      (0): Residual_block(\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(2, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (selu): SELU(inplace=True)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(2, 3), stride=(1, 1), padding=(0, 1))\n",
              "      )\n",
              "    )\n",
              "    (4): Sequential(\n",
              "      (0): Residual_block(\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(2, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (selu): SELU(inplace=True)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(2, 3), stride=(1, 1), padding=(0, 1))\n",
              "      )\n",
              "    )\n",
              "    (5): Sequential(\n",
              "      (0): Residual_block(\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(2, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (selu): SELU(inplace=True)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(2, 3), stride=(1, 1), padding=(0, 1))\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (attention): Sequential(\n",
              "    (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (1): SELU(inplace=True)\n",
              "    (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (3): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "  )\n",
              "  (GAT_layer_S): GraphAttentionLayer(\n",
              "    (att_proj): Linear(in_features=64, out_features=64, bias=True)\n",
              "    (proj_with_att): Linear(in_features=64, out_features=64, bias=True)\n",
              "    (proj_without_att): Linear(in_features=64, out_features=64, bias=True)\n",
              "    (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (input_drop): Dropout(p=0.2, inplace=False)\n",
              "    (act): SELU(inplace=True)\n",
              "  )\n",
              "  (GAT_layer_T): GraphAttentionLayer(\n",
              "    (att_proj): Linear(in_features=64, out_features=64, bias=True)\n",
              "    (proj_with_att): Linear(in_features=64, out_features=64, bias=True)\n",
              "    (proj_without_att): Linear(in_features=64, out_features=64, bias=True)\n",
              "    (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (input_drop): Dropout(p=0.2, inplace=False)\n",
              "    (act): SELU(inplace=True)\n",
              "  )\n",
              "  (HtrgGAT_layer_ST11): HtrgGraphAttentionLayer(\n",
              "    (proj_type1): Linear(in_features=64, out_features=64, bias=True)\n",
              "    (proj_type2): Linear(in_features=64, out_features=64, bias=True)\n",
              "    (att_proj): Linear(in_features=64, out_features=32, bias=True)\n",
              "    (att_projM): Linear(in_features=64, out_features=32, bias=True)\n",
              "    (proj_with_att): Linear(in_features=64, out_features=32, bias=True)\n",
              "    (proj_without_att): Linear(in_features=64, out_features=32, bias=True)\n",
              "    (proj_with_attM): Linear(in_features=64, out_features=32, bias=True)\n",
              "    (proj_without_attM): Linear(in_features=64, out_features=32, bias=True)\n",
              "    (bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (input_drop): Dropout(p=0.2, inplace=False)\n",
              "    (act): SELU(inplace=True)\n",
              "  )\n",
              "  (HtrgGAT_layer_ST12): HtrgGraphAttentionLayer(\n",
              "    (proj_type1): Linear(in_features=32, out_features=32, bias=True)\n",
              "    (proj_type2): Linear(in_features=32, out_features=32, bias=True)\n",
              "    (att_proj): Linear(in_features=32, out_features=32, bias=True)\n",
              "    (att_projM): Linear(in_features=32, out_features=32, bias=True)\n",
              "    (proj_with_att): Linear(in_features=32, out_features=32, bias=True)\n",
              "    (proj_without_att): Linear(in_features=32, out_features=32, bias=True)\n",
              "    (proj_with_attM): Linear(in_features=32, out_features=32, bias=True)\n",
              "    (proj_without_attM): Linear(in_features=32, out_features=32, bias=True)\n",
              "    (bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (input_drop): Dropout(p=0.2, inplace=False)\n",
              "    (act): SELU(inplace=True)\n",
              "  )\n",
              "  (HtrgGAT_layer_ST21): HtrgGraphAttentionLayer(\n",
              "    (proj_type1): Linear(in_features=64, out_features=64, bias=True)\n",
              "    (proj_type2): Linear(in_features=64, out_features=64, bias=True)\n",
              "    (att_proj): Linear(in_features=64, out_features=32, bias=True)\n",
              "    (att_projM): Linear(in_features=64, out_features=32, bias=True)\n",
              "    (proj_with_att): Linear(in_features=64, out_features=32, bias=True)\n",
              "    (proj_without_att): Linear(in_features=64, out_features=32, bias=True)\n",
              "    (proj_with_attM): Linear(in_features=64, out_features=32, bias=True)\n",
              "    (proj_without_attM): Linear(in_features=64, out_features=32, bias=True)\n",
              "    (bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (input_drop): Dropout(p=0.2, inplace=False)\n",
              "    (act): SELU(inplace=True)\n",
              "  )\n",
              "  (HtrgGAT_layer_ST22): HtrgGraphAttentionLayer(\n",
              "    (proj_type1): Linear(in_features=32, out_features=32, bias=True)\n",
              "    (proj_type2): Linear(in_features=32, out_features=32, bias=True)\n",
              "    (att_proj): Linear(in_features=32, out_features=32, bias=True)\n",
              "    (att_projM): Linear(in_features=32, out_features=32, bias=True)\n",
              "    (proj_with_att): Linear(in_features=32, out_features=32, bias=True)\n",
              "    (proj_without_att): Linear(in_features=32, out_features=32, bias=True)\n",
              "    (proj_with_attM): Linear(in_features=32, out_features=32, bias=True)\n",
              "    (proj_without_attM): Linear(in_features=32, out_features=32, bias=True)\n",
              "    (bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (input_drop): Dropout(p=0.2, inplace=False)\n",
              "    (act): SELU(inplace=True)\n",
              "  )\n",
              "  (pool_S): GraphPool(\n",
              "    (sigmoid): Sigmoid()\n",
              "    (proj): Linear(in_features=64, out_features=1, bias=True)\n",
              "    (drop): Dropout(p=0.3, inplace=False)\n",
              "  )\n",
              "  (pool_T): GraphPool(\n",
              "    (sigmoid): Sigmoid()\n",
              "    (proj): Linear(in_features=64, out_features=1, bias=True)\n",
              "    (drop): Dropout(p=0.3, inplace=False)\n",
              "  )\n",
              "  (pool_hS1): GraphPool(\n",
              "    (sigmoid): Sigmoid()\n",
              "    (proj): Linear(in_features=32, out_features=1, bias=True)\n",
              "    (drop): Dropout(p=0.3, inplace=False)\n",
              "  )\n",
              "  (pool_hT1): GraphPool(\n",
              "    (sigmoid): Sigmoid()\n",
              "    (proj): Linear(in_features=32, out_features=1, bias=True)\n",
              "    (drop): Dropout(p=0.3, inplace=False)\n",
              "  )\n",
              "  (pool_hS2): GraphPool(\n",
              "    (sigmoid): Sigmoid()\n",
              "    (proj): Linear(in_features=32, out_features=1, bias=True)\n",
              "    (drop): Dropout(p=0.3, inplace=False)\n",
              "  )\n",
              "  (pool_hT2): GraphPool(\n",
              "    (sigmoid): Sigmoid()\n",
              "    (proj): Linear(in_features=32, out_features=1, bias=True)\n",
              "    (drop): Dropout(p=0.3, inplace=False)\n",
              "  )\n",
              "  (out_layer): Linear(in_features=160, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download custom dataset Dataset_Speech_Assignment.zip and unzip it\n",
        "!pwd\n",
        "!cd /content/\n",
        "\n",
        "import gdown\n",
        "\n",
        "# https://drive.google.com/file/d/1K_rUC1o7BPHgZ_WgzBr64B9MO_UFn6Rb/view?usp=sharing\n",
        "\n",
        "file_id = \"1K_rUC1o7BPHgZ_WgzBr64B9MO_UFn6Rb\"  # Replace this with file's ID\n",
        "output_file = \"Dataset_Speech_Assignment.zip\"  # Replace \"data_file.ext\" with the desired output filename and extension\n",
        "\n",
        "gdown.download(f\"https://drive.google.com/uc?id={file_id}\", output_file)\n",
        "\n",
        "! unzip /content/SSL_Anti-spoofing/fairseq-a54021305d6b3c4c5959ac9395135f63202db8f1/Dataset_Speech_Assignment.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-0_89tJApaE",
        "outputId": "bc01ecc4-4eae-4099-83ba-3e0966982620"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/SSL_Anti-spoofing/fairseq-a54021305d6b3c4c5959ac9395135f63202db8f1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1K_rUC1o7BPHgZ_WgzBr64B9MO_UFn6Rb\n",
            "From (redirected): https://drive.google.com/uc?id=1K_rUC1o7BPHgZ_WgzBr64B9MO_UFn6Rb&confirm=t&uuid=b4c2f679-7e15-4d20-b372-88cfc7646800\n",
            "To: /content/SSL_Anti-spoofing/fairseq-a54021305d6b3c4c5959ac9395135f63202db8f1/Dataset_Speech_Assignment.zip\n",
            "100%|██████████| 175M/175M [00:01<00:00, 90.0MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/SSL_Anti-spoofing/fairseq-a54021305d6b3c4c5959ac9395135f63202db8f1/Dataset_Speech_Assignment.zip\n",
            "   creating: Fake/\n",
            "  inflating: __MACOSX/._Fake         \n",
            "  inflating: Fake/4afd90843b9214d26c3f2f7c695a6a1f.mp3  \n",
            "  inflating: __MACOSX/Fake/._4afd90843b9214d26c3f2f7c695a6a1f.mp3  \n",
            "  inflating: Fake/en-US-TonyNeural.mp3  \n",
            "  inflating: __MACOSX/Fake/._en-US-TonyNeural.mp3  \n",
            "  inflating: Fake/fa190735c8207c4af43cfc2c1e2a4b9b.mp3  \n",
            "  inflating: __MACOSX/Fake/._fa190735c8207c4af43cfc2c1e2a4b9b.mp3  \n",
            "  inflating: Fake/download (7).wav   \n",
            "  inflating: __MACOSX/Fake/._download (7).wav  \n",
            "  inflating: Fake/Neil-DeGrasse-Tyson-sample.wav  \n",
            "  inflating: __MACOSX/Fake/._Neil-DeGrasse-Tyson-sample.wav  \n",
            "  inflating: Fake/William+US.wav     \n",
            "  inflating: __MACOSX/Fake/._William+US.wav  \n",
            "  inflating: Fake/queen-elizabeth-ii-reads-god-save-the-queen-by-sex-pistols-speech-synthesis.wav  \n",
            "  inflating: __MACOSX/Fake/._queen-elizabeth-ii-reads-god-save-the-queen-by-sex-pistols-speech-synthesis.wav  \n",
            "  inflating: Fake/fake_femalevoicesample.wav  \n",
            "  inflating: __MACOSX/Fake/._fake_femalevoicesample.wav  \n",
            "  inflating: Fake/Susan+US.wav       \n",
            "  inflating: __MACOSX/Fake/._Susan+US.wav  \n",
            "  inflating: Fake/full_-MiaKc8PntSUM1tLRsYg.wav  \n",
            "  inflating: __MACOSX/Fake/._full_-MiaKc8PntSUM1tLRsYg.wav  \n",
            "  inflating: Fake/JR-sample.wav      \n",
            "  inflating: __MACOSX/Fake/._JR-sample.wav  \n",
            "  inflating: Fake/Obama-sample.wav   \n",
            "  inflating: __MACOSX/Fake/._Obama-sample.wav  \n",
            "  inflating: Fake/Explainer+Voices.mp3  \n",
            "  inflating: __MACOSX/Fake/._Explainer+Voices.mp3  \n",
            "  inflating: Fake/ayla_ja.wav        \n",
            "  inflating: __MACOSX/Fake/._ayla_ja.wav  \n",
            "  inflating: Fake/ayla_en.wav        \n",
            "  inflating: __MACOSX/Fake/._ayla_en.wav  \n",
            "  inflating: Fake/full_-MiaHAb7oKxrafkHwoPr.wav  \n",
            "  inflating: __MACOSX/Fake/._full_-MiaHAb7oKxrafkHwoPr.wav  \n",
            "  inflating: Fake/f2a1a3553464a18a1740cfc57c8ac867.mp3  \n",
            "  inflating: __MACOSX/Fake/._f2a1a3553464a18a1740cfc57c8ac867.mp3  \n",
            "  inflating: Fake/full_-MNf1ZIYtFlKVVR8wCQw.mp3  \n",
            "  inflating: __MACOSX/Fake/._full_-MNf1ZIYtFlKVVR8wCQw.mp3  \n",
            "  inflating: Fake/Oliver+UK.wav      \n",
            "  inflating: __MACOSX/Fake/._Oliver+UK.wav  \n",
            "  inflating: Fake/carla_en.wav       \n",
            "  inflating: __MACOSX/Fake/._carla_en.wav  \n",
            "  inflating: Fake/f18d5ed817e7fc015455cbb8c2354985.mp3  \n",
            "  inflating: __MACOSX/Fake/._f18d5ed817e7fc015455cbb8c2354985.mp3  \n",
            "  inflating: Fake/a-deepfake-singularity (1).wav  \n",
            "  inflating: __MACOSX/Fake/._a-deepfake-singularity (1).wav  \n",
            "  inflating: Fake/Jordan+US.wav      \n",
            "  inflating: __MACOSX/Fake/._Jordan+US.wav  \n",
            "  inflating: Fake/Local+Accents.mp3  \n",
            "  inflating: __MACOSX/Fake/._Local+Accents.mp3  \n",
            "  inflating: Fake/en-US-DavisNeural.mp3  \n",
            "  inflating: __MACOSX/Fake/._en-US-DavisNeural.mp3  \n",
            "  inflating: Fake/Emotions.mp3       \n",
            "  inflating: __MACOSX/Fake/._Emotions.mp3  \n",
            "  inflating: Fake/full_-MiaJSQIwiH93mWxCT8q.wav  \n",
            "  inflating: __MACOSX/Fake/._full_-MiaJSQIwiH93mWxCT8q.wav  \n",
            "  inflating: Fake/Narrative+Voices.mp3  \n",
            "  inflating: __MACOSX/Fake/._Narrative+Voices.mp3  \n",
            "  inflating: Fake/full_-MiaLh7ZOVlQb2Yixn9r.wav  \n",
            "  inflating: __MACOSX/Fake/._full_-MiaLh7ZOVlQb2Yixn9r.wav  \n",
            "  inflating: Fake/ayla_fr.wav        \n",
            "  inflating: __MACOSX/Fake/._ayla_fr.wav  \n",
            "  inflating: Fake/Conversational+Voices.mp3  \n",
            "  inflating: __MACOSX/Fake/._Conversational+Voices.mp3  \n",
            "  inflating: Fake/download (6).wav   \n",
            "  inflating: __MACOSX/Fake/._download (6).wav  \n",
            "  inflating: Fake/Fake_AIVoiceActor_multi_lingual.mp3  \n",
            "  inflating: __MACOSX/Fake/._Fake_AIVoiceActor_multi_lingual.mp3  \n",
            "  inflating: Fake/full_-MiaLWod7QmTm4nX4Yxw.wav  \n",
            "  inflating: __MACOSX/Fake/._full_-MiaLWod7QmTm4nX4Yxw.wav  \n",
            "  inflating: Fake/b12e993b5ed83e24eb76f1c3412dc5ae.mp3  \n",
            "  inflating: __MACOSX/Fake/._b12e993b5ed83e24eb76f1c3412dc5ae.mp3  \n",
            "  inflating: Fake/TomHanks.wav       \n",
            "  inflating: __MACOSX/Fake/._TomHanks.wav  \n",
            "  inflating: Fake/b668b1099147702fa94922327695e7c1.mp3  \n",
            "  inflating: __MACOSX/Fake/._b668b1099147702fa94922327695e7c1.mp3  \n",
            "  inflating: Fake/Play.ht+-+videos.wav  \n",
            "  inflating: __MACOSX/Fake/._Play.ht+-+videos.wav  \n",
            "  inflating: Fake/Frederick+UK.wav   \n",
            "  inflating: __MACOSX/Fake/._Frederick+UK.wav  \n",
            "  inflating: Fake/en-US-JasonNeural.mp3  \n",
            "  inflating: __MACOSX/Fake/._en-US-JasonNeural.mp3  \n",
            "  inflating: Fake/z-covers-we-didnt-start-the-fire-by-billy-joel-speech-synthesis.wav  \n",
            "  inflating: __MACOSX/Fake/._z-covers-we-didnt-start-the-fire-by-billy-joel-speech-synthesis.wav  \n",
            "  inflating: Fake/full_-MiaL5eChRJY36MQcht1.wav  \n",
            "  inflating: __MACOSX/Fake/._full_-MiaL5eChRJY36MQcht1.wav  \n",
            "  inflating: Fake/e977ed9e7fc683507d2213be20d322da.mp3  \n",
            "  inflating: __MACOSX/Fake/._e977ed9e7fc683507d2213be20d322da.mp3  \n",
            "  inflating: Fake/98ac13b3b78401cd97e498dff9fc181a.mp3  \n",
            "  inflating: __MACOSX/Fake/._98ac13b3b78401cd97e498dff9fc181a.mp3  \n",
            "  inflating: Fake/download (1).wav   \n",
            "  inflating: __MACOSX/Fake/._download (1).wav  \n",
            "  inflating: Fake/Fake_GlindaAIClonedVoice.mp3  \n",
            "  inflating: __MACOSX/Fake/._Fake_GlindaAIClonedVoice.mp3  \n",
            "  inflating: Fake/Fake_rock.wav      \n",
            "  inflating: __MACOSX/Fake/._Fake_rock.wav  \n",
            "  inflating: Fake/9f8c166693060e795c6dde3568aaa65c.mp3  \n",
            "  inflating: __MACOSX/Fake/._9f8c166693060e795c6dde3568aaa65c.mp3  \n",
            "  inflating: Fake/Fake_Charlotte.mp3  \n",
            "  inflating: __MACOSX/Fake/._Fake_Charlotte.mp3  \n",
            "  inflating: Fake/Training+Voices.mp3  \n",
            "  inflating: __MACOSX/Fake/._Training+Voices.mp3  \n",
            "  inflating: Fake/Fake_Daniel.mp3    \n",
            "  inflating: __MACOSX/Fake/._Fake_Daniel.mp3  \n",
            "  inflating: Fake/en-US-Standard-A.wav  \n",
            "  inflating: __MACOSX/Fake/._en-US-Standard-A.wav  \n",
            "  inflating: Fake/full_-MNf1ZsKwHKeOfJAAIkf.mp3  \n",
            "  inflating: __MACOSX/Fake/._full_-MNf1ZsKwHKeOfJAAIkf.mp3  \n",
            "  inflating: Fake/full_-MiaJyCLabOmLJv4jslu.wav  \n",
            "  inflating: __MACOSX/Fake/._full_-MiaJyCLabOmLJv4jslu.wav  \n",
            "  inflating: Fake/full_-MNf1Vxt10koB-ObvZjU.mp3  \n",
            "  inflating: __MACOSX/Fake/._full_-MNf1Vxt10koB-ObvZjU.mp3  \n",
            "  inflating: Fake/full_-MiaJ17yCwdyTA79U3l4.wav  \n",
            "  inflating: __MACOSX/Fake/._full_-MiaJ17yCwdyTA79U3l4.wav  \n",
            "  inflating: Fake/en-US-JaneNeural.mp3  \n",
            "  inflating: __MACOSX/Fake/._en-US-JaneNeural.mp3  \n",
            "  inflating: Fake/LJ050-0082_gen.wav  \n",
            "  inflating: __MACOSX/Fake/._LJ050-0082_gen.wav  \n",
            "  inflating: Fake/Dwayne-Johnson-sample.wav  \n",
            "  inflating: __MACOSX/Fake/._Dwayne-Johnson-sample.wav  \n",
            "  inflating: Fake/KevinHart.wav      \n",
            "  inflating: __MACOSX/Fake/._KevinHart.wav  \n",
            "  inflating: Fake/en-US-Standard-F.wav  \n",
            "  inflating: __MACOSX/Fake/._en-US-Standard-F.wav  \n",
            "  inflating: Fake/full_-MiaHSmdqzJqJ5O8wnAV.wav  \n",
            "  inflating: __MACOSX/Fake/._full_-MiaHSmdqzJqJ5O8wnAV.wav  \n",
            "  inflating: Fake/Children+Voices.mp3  \n",
            "  inflating: __MACOSX/Fake/._Children+Voices.mp3  \n",
            "  inflating: Fake/f01baf194fdea0329587252161bf8d35.mp3  \n",
            "  inflating: __MACOSX/Fake/._f01baf194fdea0329587252161bf8d35.mp3  \n",
            "  inflating: Fake/en-US-Standard-G.wav  \n",
            "  inflating: __MACOSX/Fake/._en-US-Standard-G.wav  \n",
            "  inflating: Fake/download.wav       \n",
            "  inflating: __MACOSX/Fake/._download.wav  \n",
            "  inflating: Fake/en-US-Wavenet-G.wav  \n",
            "  inflating: __MACOSX/Fake/._en-US-Wavenet-G.wav  \n",
            "  inflating: Fake/en-US-Wavenet-J.wav  \n",
            "  inflating: __MACOSX/Fake/._en-US-Wavenet-J.wav  \n",
            "  inflating: Fake/aa1d26cef0999d7df60121308a3ea289.mp3  \n",
            "  inflating: __MACOSX/Fake/._aa1d26cef0999d7df60121308a3ea289.mp3  \n",
            "  inflating: Fake/en-US-Standard-J.wav  \n",
            "  inflating: __MACOSX/Fake/._en-US-Standard-J.wav  \n",
            "  inflating: Fake/full_-MiaKpPcvJYhP0_4wiqV.wav  \n",
            "  inflating: __MACOSX/Fake/._full_-MiaKpPcvJYhP0_4wiqV.wav  \n",
            "  inflating: Fake/Elon-Sample.wav    \n",
            "  inflating: __MACOSX/Fake/._Elon-Sample.wav  \n",
            "  inflating: Fake/en-US-Neural2-A.wav  \n",
            "  inflating: __MACOSX/Fake/._en-US-Neural2-A.wav  \n",
            "  inflating: Fake/full_-MNf1YAjOv08Blxke9e7.mp3  \n",
            "  inflating: __MACOSX/Fake/._full_-MNf1YAjOv08Blxke9e7.mp3  \n",
            "  inflating: Fake/en-US-Wavenet-I.wav  \n",
            "  inflating: __MACOSX/Fake/._en-US-Wavenet-I.wav  \n",
            "  inflating: Fake/en-US-Standard-I.wav  \n",
            "  inflating: __MACOSX/Fake/._en-US-Standard-I.wav  \n",
            "  inflating: Fake/fake_elonmask.wav  \n",
            "  inflating: __MACOSX/Fake/._fake_elonmask.wav  \n",
            "  inflating: Fake/5b96390c6757ec91e10a23f00abddd6a.mp3  \n",
            "  inflating: __MACOSX/Fake/._5b96390c6757ec91e10a23f00abddd6a.mp3  \n",
            "  inflating: Fake/download (3).wav   \n",
            "  inflating: __MACOSX/Fake/._download (3).wav  \n",
            "  inflating: Fake/en-US-Standard-H.wav  \n",
            "  inflating: __MACOSX/Fake/._en-US-Standard-H.wav  \n",
            "  inflating: Fake/dba46c933622273eace54e6c071a6216.mp3  \n",
            "  inflating: __MACOSX/Fake/._dba46c933622273eace54e6c071a6216.mp3  \n",
            "  inflating: Fake/full_-MNf1_R0nNHpq73eg7yr.mp3  \n",
            "  inflating: __MACOSX/Fake/._full_-MNf1_R0nNHpq73eg7yr.mp3  \n",
            "  inflating: Fake/NickOfferman.wav   \n",
            "  inflating: __MACOSX/Fake/._NickOfferman.wav  \n",
            "  inflating: Fake/Fake_TiffanyAIClonedVoice.mp3  \n",
            "  inflating: __MACOSX/Fake/._Fake_TiffanyAIClonedVoice.mp3  \n",
            "  inflating: Fake/en-US-Wavenet-H.wav  \n",
            "  inflating: __MACOSX/Fake/._en-US-Wavenet-H.wav  \n",
            "  inflating: Fake/Play.ht+-+short+story.wav  \n",
            "  inflating: __MACOSX/Fake/._Play.ht+-+short+story.wav  \n",
            "  inflating: Fake/full_-MNf1Up50AJoNeQfp0_E.mp3  \n",
            "  inflating: __MACOSX/Fake/._full_-MNf1Up50AJoNeQfp0_E.mp3  \n",
            "  inflating: Fake/Hudson+US.wav      \n",
            "  inflating: __MACOSX/Fake/._Hudson+US.wav  \n",
            "  inflating: Fake/download (2).wav   \n",
            "  inflating: __MACOSX/Fake/._download (2).wav  \n",
            "  inflating: Fake/fake_mrbeast.wav   \n",
            "  inflating: __MACOSX/Fake/._fake_mrbeast.wav  \n",
            "  inflating: Fake/Frankie+UK.wav     \n",
            "  inflating: __MACOSX/Fake/._Frankie+UK.wav  \n",
            "  inflating: Fake/linus-to-musk-DEMO.mp3  \n",
            "  inflating: __MACOSX/Fake/._linus-to-musk-DEMO.mp3  \n",
            "  inflating: Fake/full_-MNf1VPqkdybbzkibu5J.mp3  \n",
            "  inflating: __MACOSX/Fake/._full_-MNf1VPqkdybbzkibu5J.mp3  \n",
            "  inflating: Fake/Character+Voices.mp3  \n",
            "  inflating: __MACOSX/Fake/._Character+Voices.mp3  \n",
            "  inflating: Fake/en-US-NancyNeural.mp3  \n",
            "  inflating: __MACOSX/Fake/._en-US-NancyNeural.mp3  \n",
            "  inflating: Fake/277effc5a338f5ed0bce01c5f2a41424.mp3  \n",
            "  inflating: __MACOSX/Fake/._277effc5a338f5ed0bce01c5f2a41424.mp3  \n",
            "  inflating: Fake/Fake_JamesAIClonedVoice.mp3  \n",
            "  inflating: __MACOSX/Fake/._Fake_JamesAIClonedVoice.mp3  \n",
            "  inflating: Fake/full_-MiaKGhnF1wi3kbUVnED.wav  \n",
            "  inflating: __MACOSX/Fake/._full_-MiaKGhnF1wi3kbUVnED.wav  \n",
            "  inflating: Fake/6b5b11163b1f6f32dbeba949eaa3b3e9.mp3  \n",
            "  inflating: __MACOSX/Fake/._6b5b11163b1f6f32dbeba949eaa3b3e9.mp3  \n",
            "  inflating: Fake/3ae86dc151041de1e2bdbf8de03f42b3.mp3  \n",
            "  inflating: __MACOSX/Fake/._3ae86dc151041de1e2bdbf8de03f42b3.mp3  \n",
            "  inflating: Fake/download (5).wav   \n",
            "  inflating: __MACOSX/Fake/._download (5).wav  \n",
            "  inflating: Fake/carla_de.wav       \n",
            "  inflating: __MACOSX/Fake/._carla_de.wav  \n",
            "  inflating: Fake/fake_obama.wav     \n",
            "  inflating: __MACOSX/Fake/._fake_obama.wav  \n",
            "  inflating: Fake/485a56b665c97e535194fc3341c610f5.mp3  \n",
            "  inflating: __MACOSX/Fake/._485a56b665c97e535194fc3341c610f5.mp3  \n",
            "  inflating: Fake/5cccd90bcdcedd5ad9799700c2d5f6fa.mp3  \n",
            "  inflating: __MACOSX/Fake/._5cccd90bcdcedd5ad9799700c2d5f6fa.mp3  \n",
            "  inflating: Fake/fake_snoopdog.wav  \n",
            "  inflating: __MACOSX/Fake/._fake_snoopdog.wav  \n",
            "  inflating: Fake/c12ca32fca735a27b8b3fbac0fc93349.mp3  \n",
            "  inflating: __MACOSX/Fake/._c12ca32fca735a27b8b3fbac0fc93349.mp3  \n",
            "  inflating: Fake/fake1_1.wav        \n",
            "  inflating: __MACOSX/Fake/._fake1_1.wav  \n",
            "  inflating: Fake/carla_pt.wav       \n",
            "  inflating: __MACOSX/Fake/._carla_pt.wav  \n",
            "  inflating: Fake/full_-MmMggzQYiUs1EbXmmEv.wav  \n",
            "  inflating: __MACOSX/Fake/._full_-MmMggzQYiUs1EbXmmEv.wav  \n",
            "  inflating: Fake/Fake_Grace.mp3     \n",
            "  inflating: __MACOSX/Fake/._Fake_Grace.mp3  \n",
            "  inflating: Fake/Fake_Fin.mp3       \n",
            "  inflating: __MACOSX/Fake/._Fake_Fin.mp3  \n",
            "  inflating: Fake/Larry+US.wav       \n",
            "  inflating: __MACOSX/Fake/._Larry+US.wav  \n",
            "  inflating: Fake/Stella+UK.wav      \n",
            "  inflating: __MACOSX/Fake/._Stella+UK.wav  \n",
            "  inflating: Fake/JFK-sample.wav     \n",
            "  inflating: __MACOSX/Fake/._JFK-sample.wav  \n",
            "  inflating: Fake/97372c76c357858eb984a5a41230f566.mp3  \n",
            "  inflating: __MACOSX/Fake/._97372c76c357858eb984a5a41230f566.mp3  \n",
            "  inflating: Fake/Fake_Dave.mp3      \n",
            "  inflating: __MACOSX/Fake/._Fake_Dave.mp3  \n",
            "  inflating: Fake/Arthur+UK.wav      \n",
            "  inflating: __MACOSX/Fake/._Arthur+UK.wav  \n",
            "  inflating: Fake/full_-MNf1Yk1-BA_-05yqhxY.mp3  \n",
            "  inflating: __MACOSX/Fake/._full_-MNf1Yk1-BA_-05yqhxY.mp3  \n",
            "  inflating: Fake/Anthony+US.wav     \n",
            "  inflating: __MACOSX/Fake/._Anthony+US.wav  \n",
            "   creating: Real/\n",
            "  inflating: __MACOSX/._Real         \n",
            "  inflating: Real/vad_04_zVVCun3C-oU.wav  \n",
            "  inflating: __MACOSX/Real/._vad_04_zVVCun3C-oU.wav  \n",
            "  inflating: Real/vad_00_lxuaQDmrBg8.wav  \n",
            "  inflating: __MACOSX/Real/._vad_00_lxuaQDmrBg8.wav  \n",
            "  inflating: Real/vad_07_4jhJPdJPorY.wav  \n",
            "  inflating: __MACOSX/Real/._vad_07_4jhJPdJPorY.wav  \n",
            "  inflating: Real/vad_00_akbwr49hP0A.wav  \n",
            "  inflating: __MACOSX/Real/._vad_00_akbwr49hP0A.wav  \n",
            "  inflating: Real/vad_00_U4px7ANlmck.wav  \n",
            "  inflating: __MACOSX/Real/._vad_00_U4px7ANlmck.wav  \n",
            "  inflating: Real/vad_11_ZWNArvn5-_Y.wav  \n",
            "  inflating: __MACOSX/Real/._vad_11_ZWNArvn5-_Y.wav  \n",
            "  inflating: Real/vad_00_vbj9tOXCBu0.wav  \n",
            "  inflating: __MACOSX/Real/._vad_00_vbj9tOXCBu0.wav  \n",
            "  inflating: Real/vad_00_mBflyZjG29U.wav  \n",
            "  inflating: __MACOSX/Real/._vad_00_mBflyZjG29U.wav  \n",
            "  inflating: Real/vad_40_4jhJPdJPorY.wav  \n",
            "  inflating: __MACOSX/Real/._vad_40_4jhJPdJPorY.wav  \n",
            "  inflating: Real/vad_00_U7fydLHVWsg.wav  \n",
            "  inflating: __MACOSX/Real/._vad_00_U7fydLHVWsg.wav  \n",
            "  inflating: Real/vad_19_vbj9tOXCBu0.wav  \n",
            "  inflating: __MACOSX/Real/._vad_19_vbj9tOXCBu0.wav  \n",
            "  inflating: Real/vad_12_lmBFzXHwkGM.wav  \n",
            "  inflating: __MACOSX/Real/._vad_12_lmBFzXHwkGM.wav  \n",
            "  inflating: Real/vad_24_vbj9tOXCBu0.wav  \n",
            "  inflating: __MACOSX/Real/._vad_24_vbj9tOXCBu0.wav  \n",
            "  inflating: Real/vad_23_4jhJPdJPorY.wav  \n",
            "  inflating: __MACOSX/Real/._vad_23_4jhJPdJPorY.wav  \n",
            "  inflating: Real/vad_16_vbj9tOXCBu0.wav  \n",
            "  inflating: __MACOSX/Real/._vad_16_vbj9tOXCBu0.wav  \n",
            "  inflating: Real/vad_20_lmBFzXHwkGM.wav  \n",
            "  inflating: __MACOSX/Real/._vad_20_lmBFzXHwkGM.wav  \n",
            "  inflating: Real/vad_07_ZWNArvn5-_Y.wav  \n",
            "  inflating: __MACOSX/Real/._vad_07_ZWNArvn5-_Y.wav  \n",
            "  inflating: Real/vad_56_4jhJPdJPorY.wav  \n",
            "  inflating: __MACOSX/Real/._vad_56_4jhJPdJPorY.wav  \n",
            "  inflating: Real/vad_11_4jhJPdJPorY.wav  \n",
            "  inflating: __MACOSX/Real/._vad_11_4jhJPdJPorY.wav  \n",
            "  inflating: Real/vad_35_4jhJPdJPorY.wav  \n",
            "  inflating: __MACOSX/Real/._vad_35_4jhJPdJPorY.wav  \n",
            "  inflating: Real/vad_36_Au_bch7bnvE.wav  \n",
            "  inflating: __MACOSX/Real/._vad_36_Au_bch7bnvE.wav  \n",
            "  inflating: Real/vad_02_D7qN0JB44ks.wav  \n",
            "  inflating: __MACOSX/Real/._vad_02_D7qN0JB44ks.wav  \n",
            "  inflating: Real/vad_32_vbj9tOXCBu0.wav  \n",
            "  inflating: __MACOSX/Real/._vad_32_vbj9tOXCBu0.wav  \n",
            "  inflating: Real/vad_08_vbj9tOXCBu0.wav  \n",
            "  inflating: __MACOSX/Real/._vad_08_vbj9tOXCBu0.wav  \n",
            "  inflating: Real/vad_48_4jhJPdJPorY.wav  \n",
            "  inflating: __MACOSX/Real/._vad_48_4jhJPdJPorY.wav  \n",
            "  inflating: Real/vad_05_D7qN0JB44ks.wav  \n",
            "  inflating: __MACOSX/Real/._vad_05_D7qN0JB44ks.wav  \n",
            "  inflating: Real/vad_32_4jhJPdJPorY.wav  \n",
            "  inflating: __MACOSX/Real/._vad_32_4jhJPdJPorY.wav  \n",
            "  inflating: Real/.DS_Store          \n",
            "  inflating: __MACOSX/Real/._.DS_Store  \n",
            "  inflating: Real/Real_TiffanyOriginalVoice.mp3  \n",
            "  inflating: __MACOSX/Real/._Real_TiffanyOriginalVoice.mp3  \n",
            "  inflating: Real/vad_16_4jhJPdJPorY.wav  \n",
            "  inflating: __MACOSX/Real/._vad_16_4jhJPdJPorY.wav  \n",
            "  inflating: Real/vad_51_4jhJPdJPorY.wav  \n",
            "  inflating: __MACOSX/Real/._vad_51_4jhJPdJPorY.wav  \n",
            "  inflating: Real/vad_11_vbj9tOXCBu0.wav  \n",
            "  inflating: __MACOSX/Real/._vad_11_vbj9tOXCBu0.wav  \n",
            "  inflating: Real/vad_00_ZWNArvn5-_Y.wav  \n",
            "  inflating: __MACOSX/Real/._vad_00_ZWNArvn5-_Y.wav  \n",
            "  inflating: Real/vad_19_4jhJPdJPorY.wav  \n",
            "  inflating: __MACOSX/Real/._vad_19_4jhJPdJPorY.wav  \n",
            "  inflating: Real/vad_24_4jhJPdJPorY.wav  \n",
            "  inflating: __MACOSX/Real/._vad_24_4jhJPdJPorY.wav  \n",
            "  inflating: Real/vad_20_SD23tzTVnKM.wav  \n",
            "  inflating: __MACOSX/Real/._vad_20_SD23tzTVnKM.wav  \n",
            "  inflating: Real/vad_32_ZWNArvn5-_Y.wav  \n",
            "  inflating: __MACOSX/Real/._vad_32_ZWNArvn5-_Y.wav  \n",
            "  inflating: Real/vad_15_lmBFzXHwkGM.wav  \n",
            "  inflating: __MACOSX/Real/._vad_15_lmBFzXHwkGM.wav  \n",
            "  inflating: Real/vad_23_vbj9tOXCBu0.wav  \n",
            "  inflating: __MACOSX/Real/._vad_23_vbj9tOXCBu0.wav  \n",
            "  inflating: Real/vad_47_4jhJPdJPorY.wav  \n",
            "  inflating: __MACOSX/Real/._vad_47_4jhJPdJPorY.wav  \n",
            "  inflating: Real/vad_16_ZWNArvn5-_Y.wav  \n",
            "  inflating: __MACOSX/Real/._vad_16_ZWNArvn5-_Y.wav  \n",
            "  inflating: Real/vad_07_vbj9tOXCBu0.wav  \n",
            "  inflating: __MACOSX/Real/._vad_07_vbj9tOXCBu0.wav  \n",
            "  inflating: Real/vad_00_4jhJPdJPorY.wav  \n",
            "  inflating: __MACOSX/Real/._vad_00_4jhJPdJPorY.wav  \n",
            "  inflating: Real/vad_00_hlzbjwPRNY4.wav  \n",
            "  inflating: __MACOSX/Real/._vad_00_hlzbjwPRNY4.wav  \n",
            "  inflating: Real/vad_03_zVVCun3C-oU.wav  \n",
            "  inflating: __MACOSX/Real/._vad_03_zVVCun3C-oU.wav  \n",
            "  inflating: Real/Central Avenue 7.wav  \n",
            "  inflating: __MACOSX/Real/._Central Avenue 7.wav  \n",
            "  inflating: Real/vad_04_D7qN0JB44ks.wav  \n",
            "  inflating: __MACOSX/Real/._vad_04_D7qN0JB44ks.wav  \n",
            "  inflating: Real/vad_33_4jhJPdJPorY.wav  \n",
            "  inflating: __MACOSX/Real/._vad_33_4jhJPdJPorY.wav  \n",
            "  inflating: Real/vad_00_8mckC8VBDHo.wav  \n",
            "  inflating: __MACOSX/Real/._vad_00_8mckC8VBDHo.wav  \n",
            "  inflating: Real/vad_00_zeKUm7LWyAo.wav  \n",
            "  inflating: __MACOSX/Real/._vad_00_zeKUm7LWyAo.wav  \n",
            "  inflating: Real/vad_18_ZWNArvn5-_Y.wav  \n",
            "  inflating: __MACOSX/Real/._vad_18_ZWNArvn5-_Y.wav  \n",
            "  inflating: Real/vad_09_vbj9tOXCBu0.wav  \n",
            "  inflating: __MACOSX/Real/._vad_09_vbj9tOXCBu0.wav  \n",
            "  inflating: Real/vad_49_4jhJPdJPorY.wav  \n",
            "  inflating: __MACOSX/Real/._vad_49_4jhJPdJPorY.wav  \n",
            "  inflating: Real/vad_02_hq7fDV3zyKg.wav  \n",
            "  inflating: __MACOSX/Real/._vad_02_hq7fDV3zyKg.wav  \n",
            "  inflating: Real/vad_50_4jhJPdJPorY.wav  \n",
            "  inflating: __MACOSX/Real/._vad_50_4jhJPdJPorY.wav  \n",
            "  inflating: Real/vad_10_vbj9tOXCBu0.wav  \n",
            "  inflating: __MACOSX/Real/._vad_10_vbj9tOXCBu0.wav  \n",
            "  inflating: Real/vad_01_ZWNArvn5-_Y.wav  \n",
            "  inflating: __MACOSX/Real/._vad_01_ZWNArvn5-_Y.wav  \n",
            "  inflating: Real/vad_17_4jhJPdJPorY.wav  \n",
            "  inflating: __MACOSX/Real/._vad_17_4jhJPdJPorY.wav  \n",
            "  inflating: Real/vad_14_Au_bch7bnvE.wav  \n",
            "  inflating: __MACOSX/Real/._vad_14_Au_bch7bnvE.wav  \n",
            "  inflating: Real/Central Avenue 6.wav  \n",
            "  inflating: __MACOSX/Real/._Central Avenue 6.wav  \n",
            "  inflating: Real/vad_22_vbj9tOXCBu0.wav  \n",
            "  inflating: __MACOSX/Real/._vad_22_vbj9tOXCBu0.wav  \n",
            "  inflating: Real/Central Avenue 4.wav  \n",
            "  inflating: __MACOSX/Real/._Central Avenue 4.wav  \n",
            "  inflating: Real/vad_21_SD23tzTVnKM.wav  \n",
            "  inflating: __MACOSX/Real/._vad_21_SD23tzTVnKM.wav  \n",
            "  inflating: Real/vad_61_Au_bch7bnvE.wav  \n",
            "  inflating: __MACOSX/Real/._vad_61_Au_bch7bnvE.wav  \n",
            "  inflating: Real/vad_80_Au_bch7bnvE.wav  \n",
            "  inflating: __MACOSX/Real/._vad_80_Au_bch7bnvE.wav  \n",
            "  inflating: Real/vad_18_4jhJPdJPorY.wav  \n",
            "  inflating: __MACOSX/Real/._vad_18_4jhJPdJPorY.wav  \n",
            "  inflating: Real/vad_01_4jhJPdJPorY.wav  \n",
            "  inflating: __MACOSX/Real/._vad_01_4jhJPdJPorY.wav  \n",
            "  inflating: Real/vad_196_Au_bch7bnvE.wav  \n",
            "  inflating: __MACOSX/Real/._vad_196_Au_bch7bnvE.wav  \n",
            "  inflating: Real/vad_02_zVVCun3C-oU.wav  \n",
            "  inflating: __MACOSX/Real/._vad_02_zVVCun3C-oU.wav  \n",
            "  inflating: Real/vad_02_Au_bch7bnvE.wav  \n",
            "  inflating: __MACOSX/Real/._vad_02_Au_bch7bnvE.wav  \n",
            "  inflating: Real/vad_01_hlzbjwPRNY4.wav  \n",
            "  inflating: __MACOSX/Real/._vad_01_hlzbjwPRNY4.wav  \n",
            "  inflating: Real/vad_46_4jhJPdJPorY.wav  \n",
            "  inflating: __MACOSX/Real/._vad_46_4jhJPdJPorY.wav  \n",
            "  inflating: Real/Central Avenue 5.wav  \n",
            "  inflating: __MACOSX/Real/._Central Avenue 5.wav  \n",
            "  inflating: Real/vad_06_vbj9tOXCBu0.wav  \n",
            "  inflating: __MACOSX/Real/._vad_06_vbj9tOXCBu0.wav  \n",
            "  inflating: Real/vad_01_vbj9tOXCBu0.wav  \n",
            "  inflating: __MACOSX/Real/._vad_01_vbj9tOXCBu0.wav  \n",
            "  inflating: Real/Central Avenue 1.wav  \n",
            "  inflating: __MACOSX/Real/._Central Avenue 1.wav  \n",
            "  inflating: Real/vad_01_mBflyZjG29U.wav  \n",
            "  inflating: __MACOSX/Real/._vad_01_mBflyZjG29U.wav  \n",
            "  inflating: Real/vad_41_4jhJPdJPorY.wav  \n",
            "  inflating: __MACOSX/Real/._vad_41_4jhJPdJPorY.wav  \n",
            "  inflating: Real/vad_05_zVVCun3C-oU.wav  \n",
            "  inflating: __MACOSX/Real/._vad_05_zVVCun3C-oU.wav  \n",
            "  inflating: Real/vad_01_akbwr49hP0A.wav  \n",
            "  inflating: __MACOSX/Real/._vad_01_akbwr49hP0A.wav  \n",
            "  inflating: Real/vad_22_4jhJPdJPorY.wav  \n",
            "  inflating: __MACOSX/Real/._vad_22_4jhJPdJPorY.wav  \n",
            "  inflating: Real/vad_58_4jhJPdJPorY.wav  \n",
            "  inflating: __MACOSX/Real/._vad_58_4jhJPdJPorY.wav  \n",
            "  inflating: Real/vad_18_vbj9tOXCBu0.wav  \n",
            "  inflating: __MACOSX/Real/._vad_18_vbj9tOXCBu0.wav  \n",
            "  inflating: Real/RealGlindaOriginalVoice.mp3  \n",
            "  inflating: __MACOSX/Real/._RealGlindaOriginalVoice.mp3  \n",
            "  inflating: Real/vad_34_ZWNArvn5-_Y.wav  \n",
            "  inflating: __MACOSX/Real/._vad_34_ZWNArvn5-_Y.wav  \n",
            "  inflating: Real/vad_25_vbj9tOXCBu0.wav  \n",
            "  inflating: __MACOSX/Real/._vad_25_vbj9tOXCBu0.wav  \n",
            "  inflating: Real/Central Avenue 2.wav  \n",
            "  inflating: __MACOSX/Real/._Central Avenue 2.wav  \n",
            "  inflating: Real/vad_10_4jhJPdJPorY.wav  \n",
            "  inflating: __MACOSX/Real/._vad_10_4jhJPdJPorY.wav  \n",
            "  inflating: Real/vad_17_vbj9tOXCBu0.wav  \n",
            "  inflating: __MACOSX/Real/._vad_17_vbj9tOXCBu0.wav  \n",
            "  inflating: Real/vad_06_ZWNArvn5-_Y.wav  \n",
            "  inflating: __MACOSX/Real/._vad_06_ZWNArvn5-_Y.wav  \n",
            "  inflating: Real/vad_57_4jhJPdJPorY.wav  \n",
            "  inflating: __MACOSX/Real/._vad_57_4jhJPdJPorY.wav  \n",
            "  inflating: Real/vad_62_bn1mwPZmRJo.wav  \n",
            "  inflating: __MACOSX/Real/._vad_62_bn1mwPZmRJo.wav  \n",
            "  inflating: Real/vad_22_ZWNArvn5-_Y.wav  \n",
            "  inflating: __MACOSX/Real/._vad_22_ZWNArvn5-_Y.wav  \n",
            "  inflating: Real/vad_09_4jhJPdJPorY.wav  \n",
            "  inflating: __MACOSX/Real/._vad_09_4jhJPdJPorY.wav  \n",
            "  inflating: Real/vad_34_4jhJPdJPorY.wav  \n",
            "  inflating: __MACOSX/Real/._vad_34_4jhJPdJPorY.wav  \n",
            "  inflating: Real/Central Avenue 3.wav  \n",
            "  inflating: __MACOSX/Real/._Central Avenue 3.wav  \n",
            "  inflating: Real/vad_03_D7qN0JB44ks.wav  \n",
            "  inflating: __MACOSX/Real/._vad_03_D7qN0JB44ks.wav  \n",
            "  inflating: Real/vad_19_lmBFzXHwkGM.wav  \n",
            "  inflating: __MACOSX/Real/._vad_19_lmBFzXHwkGM.wav  \n",
            "  inflating: Real/vad_12_vbj9tOXCBu0.wav  \n",
            "  inflating: __MACOSX/Real/._vad_12_vbj9tOXCBu0.wav  \n",
            "  inflating: Real/vad_03_ZWNArvn5-_Y.wav  \n",
            "  inflating: __MACOSX/Real/._vad_03_ZWNArvn5-_Y.wav  \n",
            "  inflating: Real/vad_119_Au_bch7bnvE.wav  \n",
            "  inflating: __MACOSX/Real/._vad_119_Au_bch7bnvE.wav  \n",
            "  inflating: Real/vad_15_4jhJPdJPorY.wav  \n",
            "  inflating: __MACOSX/Real/._vad_15_4jhJPdJPorY.wav  \n",
            "  inflating: Real/vad_00_doiBdZCkEWQ.wav  \n",
            "  inflating: __MACOSX/Real/._vad_00_doiBdZCkEWQ.wav  \n",
            "  inflating: Real/vad_31_4jhJPdJPorY.wav  \n",
            "  inflating: __MACOSX/Real/._vad_31_4jhJPdJPorY.wav  \n",
            "  inflating: Real/vad_00_hq7fDV3zyKg.wav  \n",
            "  inflating: __MACOSX/Real/._vad_00_hq7fDV3zyKg.wav  \n",
            "  inflating: Real/vad_04_akbwr49hP0A.wav  \n",
            "  inflating: __MACOSX/Real/._vad_04_akbwr49hP0A.wav  \n",
            "  inflating: Real/vad_03_4jhJPdJPorY.wav  \n",
            "  inflating: __MACOSX/Real/._vad_03_4jhJPdJPorY.wav  \n",
            "  inflating: Real/vad_00_zVVCun3C-oU.wav  \n",
            "  inflating: __MACOSX/Real/._vad_00_zVVCun3C-oU.wav  \n",
            "  inflating: Real/vad_03_hlzbjwPRNY4.wav  \n",
            "  inflating: __MACOSX/Real/._vad_03_hlzbjwPRNY4.wav  \n",
            "  inflating: Real/vad_44_4jhJPdJPorY.wav  \n",
            "  inflating: __MACOSX/Real/._vad_44_4jhJPdJPorY.wav  \n",
            "  inflating: Real/vad_15_ZWNArvn5-_Y.wav  \n",
            "  inflating: __MACOSX/Real/._vad_15_ZWNArvn5-_Y.wav  \n",
            "  inflating: Real/vad_04_vbj9tOXCBu0.wav  \n",
            "  inflating: __MACOSX/Real/._vad_04_vbj9tOXCBu0.wav  \n",
            "  inflating: Real/vad_20_vbj9tOXCBu0.wav  \n",
            "  inflating: __MACOSX/Real/._vad_20_vbj9tOXCBu0.wav  \n",
            "  inflating: Real/vad_16_lmBFzXHwkGM.wav  \n",
            "  inflating: __MACOSX/Real/._vad_16_lmBFzXHwkGM.wav  \n",
            "  inflating: Real/vad_23_SD23tzTVnKM.wav  \n",
            "  inflating: __MACOSX/Real/._vad_23_SD23tzTVnKM.wav  \n",
            "  inflating: Real/vad_24_Au_bch7bnvE.wav  \n",
            "  inflating: __MACOSX/Real/._vad_24_Au_bch7bnvE.wav  \n",
            "  inflating: Real/vad_27_4jhJPdJPorY.wav  \n",
            "  inflating: __MACOSX/Real/._vad_27_4jhJPdJPorY.wav  \n",
            "  inflating: Real/vad_28_bn1mwPZmRJo.wav  \n",
            "  inflating: __MACOSX/Real/._vad_28_bn1mwPZmRJo.wav  \n",
            "  inflating: Real/vad_23_Au_bch7bnvE.wav  \n",
            "  inflating: __MACOSX/Real/._vad_23_Au_bch7bnvE.wav  \n",
            "  inflating: Real/vad_27_vbj9tOXCBu0.wav  \n",
            "  inflating: __MACOSX/Real/._vad_27_vbj9tOXCBu0.wav  \n",
            "  inflating: Real/vad_08_lmBFzXHwkGM.wav  \n",
            "  inflating: __MACOSX/Real/._vad_08_lmBFzXHwkGM.wav  \n",
            "  inflating: Real/vad_12_ZWNArvn5-_Y.wav  \n",
            "  inflating: __MACOSX/Real/._vad_12_ZWNArvn5-_Y.wav  \n",
            "  inflating: Real/vad_03_vbj9tOXCBu0.wav  \n",
            "  inflating: __MACOSX/Real/._vad_03_vbj9tOXCBu0.wav  \n",
            "  inflating: Real/vad_00_SD23tzTVnKM.wav  \n",
            "  inflating: __MACOSX/Real/._vad_00_SD23tzTVnKM.wav  \n",
            "  inflating: Real/vad_43_4jhJPdJPorY.wav  \n",
            "  inflating: __MACOSX/Real/._vad_43_4jhJPdJPorY.wav  \n",
            "  inflating: Real/vad_03_lxuaQDmrBg8.wav  \n",
            "  inflating: __MACOSX/Real/._vad_03_lxuaQDmrBg8.wav  \n",
            "  inflating: Real/vad_04_4jhJPdJPorY.wav  \n",
            "  inflating: __MACOSX/Real/._vad_04_4jhJPdJPorY.wav  \n",
            "  inflating: Real/vad_03_akbwr49hP0A.wav  \n",
            "  inflating: __MACOSX/Real/._vad_03_akbwr49hP0A.wav  \n",
            "  inflating: Real/vad_39_4jhJPdJPorY.wav  \n",
            "  inflating: __MACOSX/Real/._vad_39_4jhJPdJPorY.wav  \n",
            "  inflating: Real/Central Avenue 9.wav  \n",
            "  inflating: __MACOSX/Real/._Central Avenue 9.wav  \n",
            "  inflating: Real/vad_31_vbj9tOXCBu0.wav  \n",
            "  inflating: __MACOSX/Real/._vad_31_vbj9tOXCBu0.wav  \n",
            "  inflating: Real/vad_07_lmBFzXHwkGM.wav  \n",
            "  inflating: __MACOSX/Real/._vad_07_lmBFzXHwkGM.wav  \n",
            "  inflating: Real/vad_20_ZWNArvn5-_Y.wav  \n",
            "  inflating: __MACOSX/Real/._vad_20_ZWNArvn5-_Y.wav  \n",
            "  inflating: Real/vad_36_4jhJPdJPorY.wav  \n",
            "  inflating: __MACOSX/Real/._vad_36_4jhJPdJPorY.wav  \n",
            "  inflating: Real/vad_01_D7qN0JB44ks.wav  \n",
            "  inflating: __MACOSX/Real/._vad_01_D7qN0JB44ks.wav  \n",
            "  inflating: Real/vad_12_4jhJPdJPorY.wav  \n",
            "  inflating: __MACOSX/Real/._vad_12_4jhJPdJPorY.wav  \n",
            "  inflating: Real/vad_15_vbj9tOXCBu0.wav  \n",
            "  inflating: __MACOSX/Real/._vad_15_vbj9tOXCBu0.wav  \n",
            "  inflating: Real/vad_04_ZWNArvn5-_Y.wav  \n",
            "  inflating: __MACOSX/Real/._vad_04_ZWNArvn5-_Y.wav  \n",
            "  inflating: Real/vad_28_vbj9tOXCBu0.wav  \n",
            "  inflating: __MACOSX/Real/._vad_28_vbj9tOXCBu0.wav  \n",
            "  inflating: Real/vad_60_bn1mwPZmRJo.wav  \n",
            "  inflating: __MACOSX/Real/._vad_60_bn1mwPZmRJo.wav  \n",
            "  inflating: Real/vad_55_4jhJPdJPorY.wav  \n",
            "  inflating: __MACOSX/Real/._vad_55_4jhJPdJPorY.wav  \n",
            "  inflating: Real/real_hindi.mp3     \n",
            "  inflating: __MACOSX/Real/._real_hindi.mp3  \n",
            "  inflating: Real/vad_26_vbj9tOXCBu0.wav  \n",
            "  inflating: __MACOSX/Real/._vad_26_vbj9tOXCBu0.wav  \n",
            "  inflating: Real/vad_21_4jhJPdJPorY.wav  \n",
            "  inflating: __MACOSX/Real/._vad_21_4jhJPdJPorY.wav  \n",
            "  inflating: Real/Real_JamesOriginalVoice.mp3  \n",
            "  inflating: __MACOSX/Real/._Real_JamesOriginalVoice.mp3  \n",
            "  inflating: Real/vad_06_zVVCun3C-oU.wav  \n",
            "  inflating: __MACOSX/Real/._vad_06_zVVCun3C-oU.wav  \n",
            "  inflating: Real/vad_06_Au_bch7bnvE.wav  \n",
            "  inflating: __MACOSX/Real/._vad_06_Au_bch7bnvE.wav  \n",
            "  inflating: Real/vad_05_4jhJPdJPorY.wav  \n",
            "  inflating: __MACOSX/Real/._vad_05_4jhJPdJPorY.wav  \n",
            "  inflating: Real/vad_02_akbwr49hP0A.wav  \n",
            "  inflating: __MACOSX/Real/._vad_02_akbwr49hP0A.wav  \n",
            "  inflating: Real/vad_38_4jhJPdJPorY.wav  \n",
            "  inflating: __MACOSX/Real/._vad_38_4jhJPdJPorY.wav  \n",
            "  inflating: Real/hindi.mp3          \n",
            "  inflating: __MACOSX/Real/._hindi.mp3  \n",
            "  inflating: Real/vad_02_mBflyZjG29U.wav  \n",
            "  inflating: __MACOSX/Real/._vad_02_mBflyZjG29U.wav  \n",
            "  inflating: Real/vad_02_vbj9tOXCBu0.wav  \n",
            "  inflating: __MACOSX/Real/._vad_02_vbj9tOXCBu0.wav  \n",
            "  inflating: Real/vad_42_4jhJPdJPorY.wav  \n",
            "  inflating: __MACOSX/Real/._vad_42_4jhJPdJPorY.wav  \n",
            "  inflating: Real/vad_37_4jhJPdJPorY.wav  \n",
            "  inflating: __MACOSX/Real/._vad_37_4jhJPdJPorY.wav  \n",
            "  inflating: Real/vad_00_D7qN0JB44ks.wav  \n",
            "  inflating: __MACOSX/Real/._vad_00_D7qN0JB44ks.wav  \n",
            "  inflating: Real/vad_34_Au_bch7bnvE.wav  \n",
            "  inflating: __MACOSX/Real/._vad_34_Au_bch7bnvE.wav  \n",
            "  inflating: Real/vad_00_WX_DCcvhA6c.wav  \n",
            "  inflating: __MACOSX/Real/._vad_00_WX_DCcvhA6c.wav  \n",
            "  inflating: Real/vad_30_vbj9tOXCBu0.wav  \n",
            "  inflating: __MACOSX/Real/._vad_30_vbj9tOXCBu0.wav  \n",
            "  inflating: Real/vad_06_lmBFzXHwkGM.wav  \n",
            "  inflating: __MACOSX/Real/._vad_06_lmBFzXHwkGM.wav  \n",
            "  inflating: Real/vad_14_vbj9tOXCBu0.wav  \n",
            "  inflating: __MACOSX/Real/._vad_14_vbj9tOXCBu0.wav  \n",
            "  inflating: Real/vad_05_ZWNArvn5-_Y.wav  \n",
            "  inflating: __MACOSX/Real/._vad_05_ZWNArvn5-_Y.wav  \n",
            "  inflating: Real/vad_29_vbj9tOXCBu0.wav  \n",
            "  inflating: __MACOSX/Real/._vad_29_vbj9tOXCBu0.wav  \n",
            "  inflating: Real/vad_61_bn1mwPZmRJo.wav  \n",
            "  inflating: __MACOSX/Real/._vad_61_bn1mwPZmRJo.wav  \n",
            "  inflating: Real/vad_54_4jhJPdJPorY.wav  \n",
            "  inflating: __MACOSX/Real/._vad_54_4jhJPdJPorY.wav  \n",
            "  inflating: Real/vad_13_4jhJPdJPorY.wav  \n",
            "  inflating: __MACOSX/Real/._vad_13_4jhJPdJPorY.wav  \n",
            "  inflating: Real/vad_14_4jhJPdJPorY.wav  \n",
            "  inflating: __MACOSX/Real/._vad_14_4jhJPdJPorY.wav  \n",
            "  inflating: Real/vad_29_4jhJPdJPorY.wav  \n",
            "  inflating: __MACOSX/Real/._vad_29_4jhJPdJPorY.wav  \n",
            "  inflating: Real/vad_53_4jhJPdJPorY.wav  \n",
            "  inflating: __MACOSX/Real/._vad_53_4jhJPdJPorY.wav  \n",
            "  inflating: Real/vad_25_lmBFzXHwkGM.wav  \n",
            "  inflating: __MACOSX/Real/._vad_25_lmBFzXHwkGM.wav  \n",
            "  inflating: Real/vad_13_vbj9tOXCBu0.wav  \n",
            "  inflating: __MACOSX/Real/._vad_13_vbj9tOXCBu0.wav  \n",
            "  inflating: Real/vad_02_ZWNArvn5-_Y.wav  \n",
            "  inflating: __MACOSX/Real/._vad_02_ZWNArvn5-_Y.wav  \n",
            "  inflating: Real/vad_49_Au_bch7bnvE.wav  \n",
            "  inflating: __MACOSX/Real/._vad_49_Au_bch7bnvE.wav  \n",
            "  inflating: Real/vad_01_hq7fDV3zyKg.wav  \n",
            "  inflating: __MACOSX/Real/._vad_01_hq7fDV3zyKg.wav  \n",
            "  inflating: Real/vad_30_4jhJPdJPorY.wav  \n",
            "  inflating: __MACOSX/Real/._vad_30_4jhJPdJPorY.wav  \n",
            "  inflating: Real/vad_45_4jhJPdJPorY.wav  \n",
            "  inflating: __MACOSX/Real/._vad_45_4jhJPdJPorY.wav  \n",
            "  inflating: Real/vad_05_vbj9tOXCBu0.wav  \n",
            "  inflating: __MACOSX/Real/._vad_05_vbj9tOXCBu0.wav  \n",
            "  inflating: Real/vad_02_4jhJPdJPorY.wav  \n",
            "  inflating: __MACOSX/Real/._vad_02_4jhJPdJPorY.wav  \n",
            "  inflating: Real/vad_01_zVVCun3C-oU.wav  \n",
            "  inflating: __MACOSX/Real/._vad_01_zVVCun3C-oU.wav  \n",
            "  inflating: Real/Gujarati.mp3       \n",
            "  inflating: __MACOSX/Real/._Gujarati.mp3  \n",
            "  inflating: Real/vad_21_vbj9tOXCBu0.wav  \n",
            "  inflating: __MACOSX/Real/._vad_21_vbj9tOXCBu0.wav  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 1- Report the AUC and EER on custom dataset.\n",
        "import os\n",
        "import torch\n",
        "import torchaudio\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "import numpy as np\n",
        "\n",
        "def load_and_preprocess_audio(file_path, target_length=64600, device=torch.device('cpu')):\n",
        "    waveform, sr = torchaudio.load(file_path)\n",
        "    waveform = waveform.mean(dim=0)  # Convert stereo to mono if necessary\n",
        "\n",
        "    if len(waveform) > target_length:\n",
        "        waveform = waveform[:target_length]  # Trim the waveform\n",
        "    elif len(waveform) < target_length:\n",
        "        pad_amount = target_length - len(waveform)  # Pad the waveform\n",
        "        waveform = torch.nn.functional.pad(waveform, (0, pad_amount))\n",
        "\n",
        "    return waveform.unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
        "\n",
        "try:\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Directory paths for fake and real audio files\n",
        "    fake_dir = '/content/SSL_Anti-spoofing/fairseq-a54021305d6b3c4c5959ac9395135f63202db8f1/Fake'\n",
        "    real_dir = '/content/SSL_Anti-spoofing/fairseq-a54021305d6b3c4c5959ac9395135f63202db8f1/Real'\n",
        "\n",
        "    # Collect all audio files and their labels\n",
        "    fake_files = [os.path.join(fake_dir, f) for f in os.listdir(fake_dir) if f.endswith(('.wav', '.mp3'))]\n",
        "    real_files = [os.path.join(real_dir, f) for f in os.listdir(real_dir) if f.endswith(('.wav', '.mp3'))]\n",
        "    all_files = fake_files + real_files\n",
        "    labels = [0] * len(fake_files) + [1] * len(real_files)  # 0 for fake, 1 for real\n",
        "\n",
        "    # Load and preprocess audio files\n",
        "    features = [load_and_preprocess_audio(f, device=device) for f in all_files]\n",
        "    features = torch.cat(features, dim=0)  # Stack features\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(features)  # Assuming the model returns logits\n",
        "        predictions = torch.sigmoid(logits).cpu().numpy()[:, 1]  # Convert logits to probabilities\n",
        "\n",
        "    print(\"Reporting the AUC and EER on custom dataset in Dataset_Speech_Assignment.zip\")\n",
        "    # Calculate AUC\n",
        "    auc = roc_auc_score(labels, predictions)\n",
        "    print('AUC:', auc)\n",
        "\n",
        "    # Calculate EER\n",
        "    fpr, tpr, thresholds = roc_curve(labels, predictions)\n",
        "    fnr = 1 - tpr\n",
        "    eer_threshold = thresholds[np.nanargmin(np.abs(fpr - fnr))]\n",
        "    eer = fpr[np.nanargmin(np.abs(fpr - fnr))]\n",
        "    print('EER:', eer)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SE4K1O25xiSE",
        "outputId": "631c13db-5f24-42bd-8a36-700e9433b81e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reporting the AUC and EER on custom dataset in Dataset_Speech_Assignment.zip\n",
            "AUC: 0.5956018518518519\n",
            "EER: 0.425\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download FOR dataset\n",
        "! pwd\n",
        "! cd /content/\n",
        "! wget https://www.eecs.yorku.ca/~bil/Datasets/for-2sec.tar.gz\n",
        "! tar -xvzf for-2sec.tar.gz\n",
        "\n",
        "# https://drive.google.com/file/d/11vFBNKzYUtWqdK358_JEygawFzmmaZjO/view?usp=drive_link"
      ],
      "metadata": {
        "id": "hNiCul6yfjaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 3 - Finetune the model on FOR dataset. [4 Marks]\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torchaudio\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch import nn, optim\n",
        "\n",
        "# Define the dataset class\n",
        "class AudioDataset(Dataset):\n",
        "    def __init__(self, directory, transform=None):\n",
        "        self.directory = directory\n",
        "        self.transform = transform\n",
        "        self.files = [os.path.join(root, f)\n",
        "                      for root, _, files in os.walk(directory)\n",
        "                      for f in files if f.endswith(('.wav', '.mp3'))]\n",
        "        self.labels = [0 if 'fake' in f else 1 for f in self.files]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        file_path = self.files[idx]\n",
        "        waveform, sample_rate = torchaudio.load(file_path)\n",
        "        if self.transform:\n",
        "            waveform = self.transform(waveform)\n",
        "        return waveform, self.labels[idx]\n",
        "\n",
        "\n",
        "class MModel(nn.Module):\n",
        "    def __init__(self,device):\n",
        "        super(MModel, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(1, 16, kernel_size=3, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc = nn.Linear(512000, 1)\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "model = MModel(device)\n",
        "\n",
        "def train_model(model, train_loader, val_loader, epochs=10, lr=0.001):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for features, labels in train_loader:\n",
        "            features, labels = features.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(features).squeeze(1)\n",
        "            loss = criterion(outputs, labels.float())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Validation step\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        for features, labels in val_loader:\n",
        "            features, labels = features.to(device), labels.to(device)\n",
        "            outputs = model(features).squeeze(1)\n",
        "            val_loss += criterion(outputs, labels.float()).item()\n",
        "\n",
        "        print(f'Epoch {epoch+1}: Train Loss: {loss.item()}, Val Loss: {val_loss / len(val_loader)}')\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_tuned = Model(device)\n",
        "\n",
        "model_tuned.load_state_dict(torch.load('/content/SSL_Anti-spoofing/fairseq-a54021305d6b3c4c5959ac9395135f63202db8f1/LA_model.pth', map_location=device))\n",
        "model_tuned.to(device)\n",
        "model_tuned.eval()\n",
        "\n",
        "# Set paths and prepare data loaders\n",
        "train_dir = '/content/SSL_Anti-spoofing/fairseq-a54021305d6b3c4c5959ac9395135f63202db8f1/for-2seconds/training'\n",
        "val_dir = '/content/SSL_Anti-spoofing/fairseq-a54021305d6b3c4c5959ac9395135f63202db8f1/for-2seconds/validation'\n",
        "train_dataset = AudioDataset(train_dir)\n",
        "val_dataset = AudioDataset(val_dir)\n",
        "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=10, shuffle=False)\n",
        "\n",
        "\n",
        "trained_model = train_model(model, train_loader, val_loader, epochs=10, lr=0.0001)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjyCgCy0jyY3",
        "outputId": "c4168e3c-edc2-482d-d3cb-bafee693bff9"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss: 0.9844993948936462, Val Loss: 0.840791761861989\n",
            "Epoch 2: Train Loss: 0.7732641100883484, Val Loss: 1.1345415720025032\n",
            "Epoch 3: Train Loss: 4.441614151000977, Val Loss: 0.6468797862371792\n",
            "Epoch 4: Train Loss: 1.9308885335922241, Val Loss: 0.9512555430455679\n",
            "Epoch 5: Train Loss: 0.4162733554840088, Val Loss: 0.6838338297689761\n",
            "Epoch 6: Train Loss: 0.12798404693603516, Val Loss: 0.7006477307357131\n",
            "Epoch 7: Train Loss: 0.1075076162815094, Val Loss: 0.9270993817479155\n",
            "Epoch 8: Train Loss: 0.22668683528900146, Val Loss: 0.7928039693790274\n",
            "Epoch 9: Train Loss: 0.2486894428730011, Val Loss: 0.7855979563280044\n",
            "Epoch 10: Train Loss: 0.22241829335689545, Val Loss: 0.8183626710526092\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install pyeer"
      ],
      "metadata": {
        "id": "yzR1TfqdraYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 4 :Report the performance using AUC and EER on For dataset. [3 Marks]\n",
        "\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "from pyeer.eer_info import get_eer_stats\n",
        "import numpy as np\n",
        "\n",
        "def evaluate_model(model, data_loader):\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "    predictions = []\n",
        "    targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for features, labels in data_loader:\n",
        "            features = features.to(device)\n",
        "            outputs = model(features).squeeze(1)\n",
        "            predictions.append(outputs.cpu().numpy())\n",
        "            targets.append(labels.cpu().numpy())\n",
        "\n",
        "    predictions = np.concatenate(predictions)\n",
        "    targets = np.concatenate(targets)\n",
        "    return predictions, targets\n",
        "\n",
        "def calculate_metrics(predictions, targets):\n",
        "    # Calculate AUC\n",
        "    auc = roc_auc_score(targets, predictions)\n",
        "\n",
        "    # Separate scores into real and fake based on targets\n",
        "    genuine_scores = predictions[targets == 1]\n",
        "    impostor_scores = predictions[targets == 0]\n",
        "\n",
        "    # Calculate EER\n",
        "    fpr, tpr, thresholds = roc_curve(targets, predictions, pos_label=1)\n",
        "    eer_stats = get_eer_stats(genuine_scores, impostor_scores)\n",
        "    eer = eer_stats.eer\n",
        "\n",
        "    return auc, eer\n",
        "\n",
        "# Set paths and prepare data loaders\n",
        "train_dir = '/content/SSL_Anti-spoofing/fairseq-a54021305d6b3c4c5959ac9395135f63202db8f1/for-2seconds/training'\n",
        "val_dir = '/content/SSL_Anti-spoofing/fairseq-a54021305d6b3c4c5959ac9395135f63202db8f1/for-2seconds/validation'\n",
        "test_dir = '/content/SSL_Anti-spoofing/fairseq-a54021305d6b3c4c5959ac9395135f63202db8f1/for-2seconds/testing'\n",
        "\n",
        "# train_dataset = AudioDataset(train_dir)\n",
        "# val_dataset = AudioDataset(val_dir)\n",
        "test_dataset = AudioDataset(test_dir)\n",
        "\n",
        "# train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
        "# val_loader = DataLoader(val_dataset, batch_size=10, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=10, shuffle=False)\n",
        "\n",
        "print(\"Reporting the performance of finetuned model using AUC and EER on For dataset.\")\n",
        "\n",
        "# Evaluate on validation set\n",
        "val_predictions, val_targets = evaluate_model(trained_model, val_loader)\n",
        "val_auc, val_eer = calculate_metrics(val_predictions, val_targets)\n",
        "print(f'Validation AUC: {val_auc}, Validation EER: {val_eer}')\n",
        "\n",
        "  # Evaluate on test set\n",
        "test_predictions, test_targets = evaluate_model(trained_model, test_loader)\n",
        "test_auc, test_eer = calculate_metrics(test_predictions, test_targets)\n",
        "print(f'Testing AUC: {test_auc}, Testing EER: {test_eer}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-YsTEeUBrCNe",
        "outputId": "b8326f1d-8823-4df5-8067-f77e8b278ee5"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reporting the performance of finetuned model using AUC and EER on For dataset.\n",
            "Validation AUC: 0.7271519291344302, Validation EER: 0.32908704883227174\n",
            "Testing AUC: 0.6681106725778546, Testing EER: 0.37683823529411764\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 5: Use the model trained on the FOR dataset to evaluate the custom dataset. Report the EER and AUC [2 Marks]\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torchaudio\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, base_directory, include_dirs=None, transform=None):\n",
        "\n",
        "        self.base_directory = base_directory\n",
        "        self.transform = transform\n",
        "        self.include_dirs = include_dirs if include_dirs is not None else ['Fake', 'Real']\n",
        "\n",
        "        self.files = []\n",
        "        self.labels = []\n",
        "\n",
        "        for subdir in self.include_dirs:\n",
        "            dir_path = os.path.join(self.base_directory, subdir)\n",
        "            for root, _, files in os.walk(dir_path):\n",
        "                for f in files:\n",
        "                    if f.endswith(('.wav', '.mp3')):\n",
        "                        full_path = os.path.join(root, f)\n",
        "                        self.files.append(full_path)\n",
        "                        self.labels.append(0 if 'fake' in f.lower() or 'fake' in root.lower() else 1)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        file_path = self.files[idx]\n",
        "        waveform, sample_rate = torchaudio.load(file_path)\n",
        "        if self.transform:\n",
        "            waveform = self.transform(waveform)\n",
        "        return waveform, self.labels[idx]\n",
        "\n",
        "\n",
        "# Set paths and prepare data loaders\n",
        "train_dir = '/content/SSL_Anti-spoofing/fairseq-a54021305d6b3c4c5959ac9395135f63202db8f1/for-2seconds/training'\n",
        "train_dataset = AudioDataset(train_dir)\n",
        "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=False)\n",
        "\n",
        "\n",
        "# Evaluate on validation set\n",
        "print(\"Report the EER and AUC Using finetuned Model on the original custom dataset\")\n",
        "predictions, targets = evaluate_model(trained_model, train_loader)\n",
        "auc, eer = calculate_metrics(predictions, targets)\n",
        "print(f'AUC : {auc}, EER: {eer}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yl7U857JuRlU",
        "outputId": "fe950b8b-3aa9-46f9-bfe1-ebad82f6df93"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Report the EER and AUC Using finetuned Model on the original custom dataset\n",
            "AUC : 0.9945691002332105, EER: 0.025508741759816565\n"
          ]
        }
      ]
    }
  ]
}